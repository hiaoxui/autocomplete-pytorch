[["__init__.pyi", "# @generated from torch/__init__.pyi.in\n\nfrom typing import List, Tuple, Optional, Union, Any, ContextManager, Callable, overload, Iterator\nfrom torch._six import inf\n\nimport builtins\n\n# These identifiers are reexported from other modules.  These modules\n# are not mypy-clean yet, so in order to use this stub file usefully\n# from mypy you will need to specify --follow-imports=silent.\n# Not all is lost: these imports still enable IDEs like PyCharm to offer\n# autocomplete.\n#\n# Note: Why does the syntax here look so strange?  Import visibility\n# rules in stubs are different from normal Python files!  You must use\n# 'from ... import ... as ...' syntax to cause an identifier to be\n# exposed (or use a wildcard); regular syntax is not exposed.\nfrom .random import set_rng_state as set_rng_state, get_rng_state as get_rng_state, \\\n    manual_seed as manual_seed, initial_seed as initial_seed, seed as seed\nfrom ._tensor_str import set_printoptions as set_printoptions\nfrom .functional import *\nfrom .serialization import save as save, load as load\nfrom .autograd import no_grad as no_grad, enable_grad as enable_grad, \\\n    set_grad_enabled as set_grad_enabled\nfrom ._ops import ops\nfrom ._classes import classes\nfrom . import autograd as autograd\nfrom . import cuda as cuda\nfrom . import optim as optim\nfrom . import nn as nn\nfrom . import multiprocessing as multiprocessing\nfrom . import sparse as sparse\nfrom . import onnx as onnx\nfrom . import jit as jit\nfrom . import hub as hub\nfrom . import random as random\nfrom . import distributions as distributions\nfrom . import testing as testing\nfrom . import quantization as quantization\nfrom . import __config__ as __config__\nfrom . import __future__ as __future__\n\nclass dtype: ...\n\nclass layout: ...\n\nstrided : layout = ...\n\nclass memory_format: ...\n\ncontiguous_format: memory_format = ...\n\nclass qscheme: ...\n\nper_tensor_affine: qscheme = ...\n\n# See https://github.com/python/mypy/issues/4146 for why these workarounds\n# is necessary\n_int = builtins.int\n_float = builtins.float\n_bool = builtins.bool\n\nclass device:\n    type: str\n    index: _int\n\n    @overload\n    def __init__(self, device: Union[_int, str]) -> None: ...\n\n    @overload\n    def __init__(self, type: str, index: _int) -> None: ...\n\nclass Size(Tuple[_int, ...]): ...\n\nclass Storage: ...\n\n# See https://github.com/python/mypy/issues/4146 for why these workarounds\n# is necessary\n_dtype = dtype\n_device = device\n_qscheme = qscheme\n_size = Union[Size, List[_int], Tuple[_int, ...]]\n_layout = layout\n\n# Meta-type for \"numeric\" things; matches our docs\nNumber = Union[builtins.int, builtins.float, builtins.bool]\n\nclass Generator:\n    device: _device = ...\n\n    @overload\n    def __init__(self, device: Optional[_device]=None) -> None: ...\n\n    @overload\n    def __init__(self, device: Union[_int, str]) -> None: ...\n\n# TODO: One downside of doing it this way, is direct use of\n# torch.tensor.Tensor doesn't get type annotations.  Nobody\n# should really do that, so maybe this is not so bad.\nclass Tensor:\n    requires_grad: _bool = ...\n    grad: Optional[Tensor] = ...\n    data: Tensor = ...\n    names: List[str] = ...\n    @property\n    def dtype(self) -> _dtype: ...\n    @property\n    def shape(self) -> Size: ...\n    @property\n    def device(self) -> _device: ...\n    @property\n    def T(self) -> Tensor: ...\n    @property\n    def grad_fn(self) -> Optional[Any]: ...\n    @property\n    def ndim(self) -> _int: ...\n    @property\n    def layout(self) -> _layout: ...\n\n    def __abs__(self) -> Tensor: ...\n    def __add__(self, other: Any) -> Tensor: ...\n    @overload\n    def __and__(self, other: Number) -> Tensor: ...\n    @overload\n    def __and__(self, other: Tensor) -> Tensor: ...\n    @overload\n    def __and__(self, other: Any) -> Tensor: ...\n    def __bool__(self) -> builtins.bool: ...\n    def __div__(self, other: Any) -> Tensor: ...\n    def __eq__(self, other: Any) -> Tensor: ...  # type: ignore\n    def __float__(self) -> builtins.float: ...\n    def __floordiv__(self, other: Any) -> Tensor: ...\n    def __ge__(self, other: Any) -> Tensor: ...  # type: ignore\n    def __getitem__(self, indices: Union[None, _int, slice, Tensor, List, Tuple]) -> Tensor: ...\n    def __gt__(self, other: Any) -> Tensor: ...  # type: ignore\n    def __iadd__(self, other: Any) -> Tensor: ...\n    @overload\n    def __iand__(self, other: Number) -> Tensor: ...\n    @overload\n    def __iand__(self, other: Tensor) -> Tensor: ...\n    @overload\n    def __iand__(self, other: Any) -> Tensor: ...\n    def __idiv__(self, other: Any) -> Tensor: ...\n    @overload\n    def __ilshift__(self, other: Number) -> Tensor: ...\n    @overload\n    def __ilshift__(self, other: Tensor) -> Tensor: ...\n    @overload\n    def __ilshift__(self, other: Any) -> Tensor: ...\n    def __imul__(self, other: Any) -> Tensor: ...\n    def __index__(self) -> builtins.int: ...\n    def __int__(self) -> builtins.int: ...\n    def __invert__(self) -> Tensor: ...\n    @overload\n    def __ior__(self, other: Number) -> Tensor: ...\n    @overload\n    def __ior__(self, other: Tensor) -> Tensor: ...\n    @overload\n    def __ior__(self, other: Any) -> Tensor: ...\n    @overload\n    def __irshift__(self, other: Number) -> Tensor: ...\n    @overload\n    def __irshift__(self, other: Tensor) -> Tensor: ...\n    @overload\n    def __irshift__(self, other: Any) -> Tensor: ...\n    def __isub__(self, other: Any) -> Tensor: ...\n    def __itruediv__(self, other: Any) -> Tensor: ...\n    @overload\n    def __ixor__(self, other: Number) -> Tensor: ...\n    @overload\n    def __ixor__(self, other: Tensor) -> Tensor: ...\n    @overload\n    def __ixor__(self, other: Any) -> Tensor: ...\n    def __le__(self, other: Any) -> Tensor: ...  # type: ignore\n    def __long__(self) -> builtins.int: ...\n    @overload\n    def __lshift__(self, other: Number) -> Tensor: ...\n    @overload\n    def __lshift__(self, other: Tensor) -> Tensor: ...\n    @overload\n    def __lshift__(self, other: Any) -> Tensor: ...\n    def __lt__(self, other: Any) -> Tensor: ...  # type: ignore\n    def __matmul__(self, other: Any) -> Tensor: ...\n    def __mod__(self, other: Any) -> Tensor: ...\n    def __mul__(self, other: Any) -> Tensor: ...\n    def __ne__(self, other: Any) -> Tensor: ...  # type: ignore\n    def __neg__(self) -> Tensor: ...\n    def __nonzero__(self) -> builtins.bool: ...\n    @overload\n    def __or__(self, other: Number) -> Tensor: ...\n    @overload\n    def __or__(self, other: Tensor) -> Tensor: ...\n    @overload\n    def __or__(self, other: Any) -> Tensor: ...\n    def __pow__(self, other: Any) -> Tensor: ...\n    def __radd__(self, other: Any) -> Tensor: ...\n    def __rfloordiv__(self, other: Any) -> Tensor: ...\n    def __rmul__(self, other: Any) -> Tensor: ...\n    def __rpow__(self, other: Any) -> Tensor: ...\n    @overload\n    def __rshift__(self, other: Number) -> Tensor: ...\n    @overload\n    def __rshift__(self, other: Tensor) -> Tensor: ...\n    @overload\n    def __rshift__(self, other: Any) -> Tensor: ...\n    def __rsub__(self, other: Any) -> Tensor: ...\n    def __rtruediv__(self, other: Any) -> Tensor: ...\n    def __setitem__(self, indices: Union[None, _int, slice, Tensor, List, Tuple], val: Union[Tensor, Number]) -> None: ...\n    def __sub__(self, other: Any) -> Tensor: ...\n    def __truediv__(self, other: Any) -> Tensor: ...\n    @overload\n    def __xor__(self, other: Number) -> Tensor: ...\n    @overload\n    def __xor__(self, other: Tensor) -> Tensor: ...\n    @overload\n    def __xor__(self, other: Any) -> Tensor: ...\n    def _coalesced_(self, coalesced: _bool) -> Tensor: ...\n    def _dimI(self) -> _int: ...\n    def _dimV(self) -> _int: ...\n    def _indices(self) -> Tensor: ...\n    def _nnz(self) -> _int: ...\n    def _values(self) -> Tensor: ...\n    def abs(self) -> Tensor: ...\n    def abs_(self) -> Tensor: ...\n    def acos(self) -> Tensor: ...\n    def acos_(self) -> Tensor: ...\n    def add(self, other: Union[Tensor, Number], *, alpha: Optional[Number]=1, out: Optional[Tensor]=None) -> Tensor: ...\n    def add_(self, other: Union[Tensor, Number], *, alpha: Optional[Number]=1) -> Tensor: ...\n    def addbmm(self, batch1: Tensor, batch2: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...\n    def addbmm_(self, batch1: Tensor, batch2: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...\n    def addcdiv(self, tensor1: Tensor, tensor2: Tensor, *, value: Number=1) -> Tensor: ...\n    def addcdiv_(self, tensor1: Tensor, tensor2: Tensor, *, value: Number=1) -> Tensor: ...\n    def addcmul(self, tensor1: Tensor, tensor2: Tensor, *, value: Number=1) -> Tensor: ...\n    def addcmul_(self, tensor1: Tensor, tensor2: Tensor, *, value: Number=1) -> Tensor: ...\n    def addmm(self, mat1: Tensor, mat2: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...\n    def addmm_(self, mat1: Tensor, mat2: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...\n    def addmv(self, mat: Tensor, vec: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...\n    def addmv_(self, mat: Tensor, vec: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...\n    def addr(self, vec1: Tensor, vec2: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...\n    def addr_(self, vec1: Tensor, vec2: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...\n    def align_as(self, other: Tensor) -> Tensor: ...\n    @overload\n    def align_to(self, names: List[Union[str, None]]) -> Tensor: ...\n    @overload\n    def align_to(self, order: List[Union[str, None]], ellipsis_idx: _int) -> Tensor: ...\n    @overload\n    def all(self, dim: _int, keepdim: _bool=False) -> Tensor: ...\n    @overload\n    def all(self, dim: Union[str, None], keepdim: _bool=False) -> Tensor: ...\n    @overload\n    def all(self) -> Tensor: ...\n    def allclose(self, other: Tensor, rtol: _float=1e-05, atol: _float=1e-08, equal_nan: _bool=False) -> _bool: ...\n    def angle(self) -> Tensor: ...\n    @overload\n    def any(self, dim: _int, keepdim: _bool=False) -> Tensor: ...\n    @overload\n    def any(self, dim: Union[str, None], keepdim: _bool=False) -> Tensor: ...\n    @overload\n    def any(self) -> Tensor: ...\n    def apply_(self, callable: Callable) -> Tensor: ...\n    def argmax(self, dim: Optional[_int]=None, keepdim: _bool=False) -> Tensor: ...\n    def argmin(self, dim: Optional[_int]=None, keepdim: _bool=False) -> Tensor: ...\n    @overload\n    def argsort(self, dim: _int=-1, descending: _bool=False) -> Tensor: ...\n    @overload\n    def argsort(self, dim: Union[str, None], descending: _bool=False) -> Tensor: ...\n    def as_strided(self, size: _size, stride: _size, storage_offset: Optional[_int]=None) -> Tensor: ...\n    def as_strided_(self, size: _size, stride: _size, storage_offset: Optional[_int]=None) -> Tensor: ...\n    def asin(self) -> Tensor: ...\n    def asin_(self) -> Tensor: ...\n    def atan(self) -> Tensor: ...\n    def atan2(self, other: Tensor) -> Tensor: ...\n    def atan2_(self, other: Tensor) -> Tensor: ...\n    def atan_(self) -> Tensor: ...\n    def backward(self, gradient: Optional[Tensor]=None, keep_graph: _bool=False, create_graph: _bool=False) -> None: ...\n    def baddbmm(self, batch1: Tensor, batch2: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...\n    def baddbmm_(self, batch1: Tensor, batch2: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...\n    @overload\n    def bernoulli(self, *, generator: Generator=None) -> Tensor: ...\n    @overload\n    def bernoulli(self, p: _float, *, generator: Generator=None) -> Tensor: ...\n    @overload\n    def bernoulli_(self, p: Tensor, *, generator: Generator=None) -> Tensor: ...\n    @overload\n    def bernoulli_(self, p: _float=0.5, *, generator: Generator=None) -> Tensor: ...\n    def bfloat16(self) -> Tensor: ...\n    def bincount(self, weights: Optional[Tensor]=None, minlength: _int=0) -> Tensor: ...\n    @overload\n    def bitwise_and(self, other: Number) -> Tensor: ...\n    @overload\n    def bitwise_and(self, other: Tensor) -> Tensor: ...\n    @overload\n    def bitwise_and_(self, other: Number) -> Tensor: ...\n    @overload\n    def bitwise_and_(self, other: Tensor) -> Tensor: ...\n    def bitwise_not(self) -> Tensor: ...\n    def bitwise_not_(self) -> Tensor: ...\n    @overload\n    def bitwise_or(self, other: Number) -> Tensor: ...\n    @overload\n    def bitwise_or(self, other: Tensor) -> Tensor: ...\n    @overload\n    def bitwise_or_(self, other: Number) -> Tensor: ...\n    @overload\n    def bitwise_or_(self, other: Tensor) -> Tensor: ...\n    @overload\n    def bitwise_xor(self, other: Number) -> Tensor: ...\n    @overload\n    def bitwise_xor(self, other: Tensor) -> Tensor: ...\n    @overload\n    def bitwise_xor_(self, other: Number) -> Tensor: ...\n    @overload\n    def bitwise_xor_(self, other: Tensor) -> Tensor: ...\n    def bmm(self, mat2: Tensor) -> Tensor: ...\n    def bool(self) -> Tensor: ...\n    def byte(self) -> Tensor: ...\n    def cauchy_(self, median: _float=0, sigma: _float=1, *, generator: Generator=None) -> Tensor: ...\n    def ceil(self) -> Tensor: ...\n    def ceil_(self) -> Tensor: ...\n    def char(self) -> Tensor: ...\n    def cholesky(self, upper: _bool=False) -> Tensor: ...\n    def cholesky_inverse(self, upper: _bool=False) -> Tensor: ...\n    def cholesky_solve(self, input2: Tensor, upper: _bool=False) -> Tensor: ...\n    def chunk(self, chunks: _int, dim: _int=0) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...\n    def clamp(self, min: _float=-inf, max: _float=inf, *, out: Optional[Tensor]=None) -> Tensor: ...\n    def clamp_(self, min: _float=-inf, max: _float=inf) -> Tensor: ...\n    def clamp_max(self, max: Number) -> Tensor: ...\n    def clamp_max_(self, max: Number) -> Tensor: ...\n    def clamp_min(self, min: Number) -> Tensor: ...\n    def clamp_min_(self, min: Number) -> Tensor: ...\n    def clone(self, *, memory_format: Optional[memory_format]=None) -> Tensor: ...\n    def coalesce(self) -> Tensor: ...\n    def conj(self) -> Tensor: ...\n    def contiguous(self) -> Tensor: ...\n    def cos(self) -> Tensor: ...\n    def cos_(self) -> Tensor: ...\n    def cosh(self) -> Tensor: ...\n    def cosh_(self) -> Tensor: ...\n    def cpu(self) -> Tensor: ...\n    def cross(self, other: Tensor, dim: Optional[_int]=None) -> Tensor: ...\n    def cuda(self, device: Optional[Union[_device, _int, str]]=None, non_blocking: _bool=False) -> Tensor: ...\n    @overload\n    def cummax(self, dim: _int) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def cummax(self, dim: Union[str, None]) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def cummin(self, dim: _int) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def cummin(self, dim: Union[str, None]) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def cumprod(self, dim: _int, *, dtype: Optional[_dtype]=None) -> Tensor: ...\n    @overload\n    def cumprod(self, dim: Union[str, None], *, dtype: Optional[_dtype]=None) -> Tensor: ...\n    @overload\n    def cumsum(self, dim: _int, *, dtype: Optional[_dtype]=None) -> Tensor: ...\n    @overload\n    def cumsum(self, dim: Union[str, None], *, dtype: Optional[_dtype]=None) -> Tensor: ...\n    def dense_dim(self) -> _int: ...\n    def dequantize(self) -> Tensor: ...\n    def det(self) -> Tensor: ...\n    def detach(self) -> Tensor: ...\n    def detach_(self) -> Tensor: ...\n    def diag(self, diagonal: _int=0) -> Tensor: ...\n    def diag_embed(self, offset: _int=0, dim1: _int=-2, dim2: _int=-1) -> Tensor: ...\n    def diagflat(self, offset: _int=0) -> Tensor: ...\n    @overload\n    def diagonal(self, offset: _int=0, dim1: _int=0, dim2: _int=1) -> Tensor: ...\n    @overload\n    def diagonal(self, *, outdim: Union[str, None], dim1: Union[str, None], dim2: Union[str, None], offset: _int=0) -> Tensor: ...\n    def digamma(self) -> Tensor: ...\n    def digamma_(self) -> Tensor: ...\n    def dim(self) -> _int: ...\n    def dist(self, other: Tensor, p: Number=2) -> Tensor: ...\n    def div(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    def div_(self, other: Union[Tensor, Number]) -> Tensor: ...\n    def dot(self, tensor: Tensor) -> Tensor: ...\n    def double(self) -> Tensor: ...\n    def eig(self, eigenvectors: _bool=False) -> Tuple[Tensor, Tensor]: ...\n    def element_size(self) -> _int: ...\n    @overload\n    def eq(self, other: Number) -> Tensor: ...\n    @overload\n    def eq(self, other: Tensor) -> Tensor: ...\n    @overload\n    def eq_(self, other: Number) -> Tensor: ...\n    @overload\n    def eq_(self, other: Tensor) -> Tensor: ...\n    def equal(self, other: Tensor) -> _bool: ...\n    def erf(self) -> Tensor: ...\n    def erf_(self) -> Tensor: ...\n    def erfc(self) -> Tensor: ...\n    def erfc_(self) -> Tensor: ...\n    def erfinv(self) -> Tensor: ...\n    def erfinv_(self) -> Tensor: ...\n    def exp(self) -> Tensor: ...\n    def exp_(self) -> Tensor: ...\n    @overload\n    def expand(self, size: _size, *, implicit: _bool=False) -> Tensor: ...\n    @overload\n    def expand(self, *size: _int, implicit: _bool=False) -> Tensor: ...\n    def expand_as(self, other: Tensor) -> Tensor: ...\n    def expm1(self) -> Tensor: ...\n    def expm1_(self) -> Tensor: ...\n    def exponential_(self, lambd: _float=1, *, generator: Generator=None) -> Tensor: ...\n    def fft(self, signal_ndim: _int, normalized: _bool=False) -> Tensor: ...\n    @overload\n    def fill_(self, value: Number) -> Tensor: ...\n    @overload\n    def fill_(self, value: Tensor) -> Tensor: ...\n    def fill_diagonal_(self, fill_value: Number, wrap: _bool=False) -> Tensor: ...\n    @overload\n    def flatten(self, start_dim: _int=0, end_dim: _int=-1) -> Tensor: ...\n    @overload\n    def flatten(self, start_dim: _int, end_dim: _int, out_dim: Union[str, None]) -> Tensor: ...\n    @overload\n    def flatten(self, start_dim: Union[str, None], end_dim: Union[str, None], out_dim: Union[str, None]) -> Tensor: ...\n    @overload\n    def flatten(self, dims: List[Union[str, None]], out_dim: Union[str, None]) -> Tensor: ...\n    @overload\n    def flip(self, dims: _size) -> Tensor: ...\n    @overload\n    def flip(self, *dims: _int) -> Tensor: ...\n    def float(self) -> Tensor: ...\n    def floor(self) -> Tensor: ...\n    def floor_(self) -> Tensor: ...\n    def floor_divide(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    def floor_divide_(self, other: Union[Tensor, Number]) -> Tensor: ...\n    @overload\n    def fmod(self, other: Number) -> Tensor: ...\n    @overload\n    def fmod(self, other: Tensor) -> Tensor: ...\n    @overload\n    def fmod_(self, other: Number) -> Tensor: ...\n    @overload\n    def fmod_(self, other: Tensor) -> Tensor: ...\n    def frac(self) -> Tensor: ...\n    def frac_(self) -> Tensor: ...\n    @overload\n    def gather(self, dim: _int, index: Tensor, *, sparse_grad: _bool=False) -> Tensor: ...\n    @overload\n    def gather(self, dim: Union[str, None], index: Tensor, *, sparse_grad: _bool=False) -> Tensor: ...\n    @overload\n    def ge(self, other: Number) -> Tensor: ...\n    @overload\n    def ge(self, other: Tensor) -> Tensor: ...\n    @overload\n    def ge_(self, other: Number) -> Tensor: ...\n    @overload\n    def ge_(self, other: Tensor) -> Tensor: ...\n    def geometric_(self, p: _float, *, generator: Generator=None) -> Tensor: ...\n    def geqrf(self) -> Tuple[Tensor, Tensor]: ...\n    def ger(self, vec2: Tensor) -> Tensor: ...\n    def get_device(self) -> _int: ...\n    @overload\n    def gt(self, other: Number) -> Tensor: ...\n    @overload\n    def gt(self, other: Tensor) -> Tensor: ...\n    @overload\n    def gt_(self, other: Number) -> Tensor: ...\n    @overload\n    def gt_(self, other: Tensor) -> Tensor: ...\n    def half(self) -> Tensor: ...\n    def hardshrink(self, lambd: Number=0.5) -> Tensor: ...\n    def histc(self, bins: _int=100, min: Number=0, max: Number=0) -> Tensor: ...\n    def ifft(self, signal_ndim: _int, normalized: _bool=False) -> Tensor: ...\n    @overload\n    def index_add(self, dim: _int, index: Tensor, source: Tensor) -> Tensor: ...\n    @overload\n    def index_add(self, dim: Union[str, None], index: Tensor, source: Tensor) -> Tensor: ...\n    def index_add_(self, dim: _int, index: Tensor, source: Tensor) -> Tensor: ...\n    @overload\n    def index_copy(self, dim: _int, index: Tensor, source: Tensor) -> Tensor: ...\n    @overload\n    def index_copy(self, dim: Union[str, None], index: Tensor, source: Tensor) -> Tensor: ...\n    @overload\n    def index_copy_(self, dim: _int, index: Tensor, source: Tensor) -> Tensor: ...\n    @overload\n    def index_copy_(self, dim: Union[str, None], index: Tensor, source: Tensor) -> Tensor: ...\n    @overload\n    def index_fill(self, dim: _int, index: Tensor, value: Number) -> Tensor: ...\n    @overload\n    def index_fill(self, dim: _int, index: Tensor, value: Tensor) -> Tensor: ...\n    @overload\n    def index_fill(self, dim: Union[str, None], index: Tensor, value: Number) -> Tensor: ...\n    @overload\n    def index_fill(self, dim: Union[str, None], index: Tensor, value: Tensor) -> Tensor: ...\n    @overload\n    def index_fill_(self, dim: _int, index: Tensor, value: Number) -> Tensor: ...\n    @overload\n    def index_fill_(self, dim: _int, index: Tensor, value: Tensor) -> Tensor: ...\n    @overload\n    def index_fill_(self, dim: Union[str, None], index: Tensor, value: Number) -> Tensor: ...\n    @overload\n    def index_fill_(self, dim: Union[str, None], index: Tensor, value: Tensor) -> Tensor: ...\n    def index_put(self, indices: Optional[Union[Tuple[Tensor, ...], List[Tensor]]], values: Tensor, accumulate: _bool=False) -> Tensor: ...\n    def index_put_(self, indices: Optional[Union[Tuple[Tensor, ...], List[Tensor]]], values: Tensor, accumulate: _bool=False) -> Tensor: ...\n    @overload\n    def index_select(self, dim: _int, index: Tensor) -> Tensor: ...\n    @overload\n    def index_select(self, dim: Union[str, None], index: Tensor) -> Tensor: ...\n    def indices(self) -> Tensor: ...\n    def int(self) -> Tensor: ...\n    def int_repr(self) -> Tensor: ...\n    def inverse(self) -> Tensor: ...\n    def irfft(self, signal_ndim: _int, normalized: _bool=False, onesided: _bool=True, signal_sizes: _size=()) -> Tensor: ...\n    def is_coalesced(self) -> _bool: ...\n    def is_complex(self) -> _bool: ...\n    def is_contiguous(self) -> _bool: ...\n    is_cuda: _bool\n    def is_distributed(self) -> _bool: ...\n    def is_floating_point(self) -> _bool: ...\n    is_leaf: _bool\n    is_mkldnn: _bool\n    def is_nonzero(self) -> _bool: ...\n    def is_pinned(self) -> _bool: ...\n    is_quantized: _bool\n    def is_same_size(self, other: Tensor) -> _bool: ...\n    def is_set_to(self, tensor: Tensor) -> _bool: ...\n    def is_signed(self) -> _bool: ...\n    is_sparse: _bool\n    def isclose(self, other: Tensor, rtol: _float=1e-05, atol: _float=1e-08, equal_nan: _bool=False) -> Tensor: ...\n    def item(self) -> Number: ...\n    @overload\n    def kthvalue(self, k: _int, dim: _int=-1, keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def kthvalue(self, k: _int, dim: Union[str, None], keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def le(self, other: Number) -> Tensor: ...\n    @overload\n    def le(self, other: Tensor) -> Tensor: ...\n    @overload\n    def le_(self, other: Number) -> Tensor: ...\n    @overload\n    def le_(self, other: Tensor) -> Tensor: ...\n    @overload\n    def lerp(self, end: Tensor, weight: Number) -> Tensor: ...\n    @overload\n    def lerp(self, end: Tensor, weight: Tensor) -> Tensor: ...\n    @overload\n    def lerp_(self, end: Tensor, weight: Number) -> Tensor: ...\n    @overload\n    def lerp_(self, end: Tensor, weight: Tensor) -> Tensor: ...\n    def lgamma(self) -> Tensor: ...\n    def lgamma_(self) -> Tensor: ...\n    def log(self) -> Tensor: ...\n    def log10(self) -> Tensor: ...\n    def log10_(self) -> Tensor: ...\n    def log1p(self) -> Tensor: ...\n    def log1p_(self) -> Tensor: ...\n    def log2(self) -> Tensor: ...\n    def log2_(self) -> Tensor: ...\n    def log_(self) -> Tensor: ...\n    def log_normal_(self, mean: _float=1, std: _float=2, *, generator: Generator=None) -> Tensor: ...\n    @overload\n    def log_softmax(self, dim: _int, dtype: Optional[_dtype]=None) -> Tensor: ...\n    @overload\n    def log_softmax(self, dim: Union[str, None], *, dtype: Optional[_dtype]=None) -> Tensor: ...\n    def logdet(self) -> Tensor: ...\n    def logical_and(self, other: Tensor) -> Tensor: ...\n    def logical_and_(self, other: Tensor) -> Tensor: ...\n    def logical_not(self) -> Tensor: ...\n    def logical_not_(self) -> Tensor: ...\n    def logical_or(self, other: Tensor) -> Tensor: ...\n    def logical_or_(self, other: Tensor) -> Tensor: ...\n    def logical_xor(self, other: Tensor) -> Tensor: ...\n    def logical_xor_(self, other: Tensor) -> Tensor: ...\n    @overload\n    def logsumexp(self, dim: Union[_int, _size], keepdim: _bool=False) -> Tensor: ...\n    @overload\n    def logsumexp(self, dim: List[Union[str, None]], keepdim: _bool=False) -> Tensor: ...\n    def long(self) -> Tensor: ...\n    def lstsq(self, A: Tensor) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def lt(self, other: Number) -> Tensor: ...\n    @overload\n    def lt(self, other: Tensor) -> Tensor: ...\n    @overload\n    def lt_(self, other: Number) -> Tensor: ...\n    @overload\n    def lt_(self, other: Tensor) -> Tensor: ...\n    def lu_solve(self, LU_data: Tensor, LU_pivots: Tensor) -> Tensor: ...\n    def map_(self, tensor: Tensor, callable: Callable) -> Tensor: ...\n    @overload\n    def masked_fill(self, mask: Tensor, value: Number) -> Tensor: ...\n    @overload\n    def masked_fill(self, mask: Tensor, value: Tensor) -> Tensor: ...\n    @overload\n    def masked_fill_(self, mask: Tensor, value: Number) -> Tensor: ...\n    @overload\n    def masked_fill_(self, mask: Tensor, value: Tensor) -> Tensor: ...\n    def masked_scatter(self, mask: Tensor, source: Tensor) -> Tensor: ...\n    def masked_scatter_(self, mask: Tensor, source: Tensor) -> Tensor: ...\n    def masked_select(self, mask: Tensor) -> Tensor: ...\n    def matmul(self, other: Tensor) -> Tensor: ...\n    def matrix_power(self, n: _int) -> Tensor: ...\n    @overload\n    def max(self, dim: _int, keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def max(self, dim: Union[str, None], keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def max(self, other: Tensor) -> Tensor: ...\n    @overload\n    def max(self) -> Tensor: ...\n    @overload\n    def mean(self, *, dtype: Optional[_dtype]=None) -> Tensor: ...\n    @overload\n    def mean(self, dim: Union[_int, _size], keepdim: _bool=False, *, dtype: Optional[_dtype]=None) -> Tensor: ...\n    @overload\n    def mean(self, dim: List[Union[str, None]], keepdim: _bool=False, *, dtype: Optional[_dtype]=None) -> Tensor: ...\n    @overload\n    def median(self, dim: _int, keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def median(self, dim: Union[str, None], keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def median(self) -> Tensor: ...\n    @overload\n    def min(self, dim: _int, keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def min(self, dim: Union[str, None], keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def min(self, other: Tensor) -> Tensor: ...\n    @overload\n    def min(self) -> Tensor: ...\n    def mm(self, mat2: Tensor) -> Tensor: ...\n    @overload\n    def mode(self, dim: _int=-1, keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def mode(self, dim: Union[str, None], keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...\n    def mul(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    def mul_(self, other: Union[Tensor, Number]) -> Tensor: ...\n    def multinomial(self, num_samples: _int, replacement: _bool=False, *, generator: Generator=None) -> Tensor: ...\n    def mv(self, vec: Tensor) -> Tensor: ...\n    def mvlgamma(self, p: _int) -> Tensor: ...\n    def mvlgamma_(self, p: _int) -> Tensor: ...\n    @overload\n    def narrow(self, dim: _int, start: _int, length: _int) -> Tensor: ...\n    @overload\n    def narrow(self, dim: _int, start: Tensor, length: _int) -> Tensor: ...\n    def narrow_copy(self, dim: _int, start: _int, length: _int) -> Tensor: ...\n    def ndimension(self) -> _int: ...\n    @overload\n    def ne(self, other: Number) -> Tensor: ...\n    @overload\n    def ne(self, other: Tensor) -> Tensor: ...\n    @overload\n    def ne_(self, other: Number) -> Tensor: ...\n    @overload\n    def ne_(self, other: Tensor) -> Tensor: ...\n    def neg(self) -> Tensor: ...\n    def neg_(self) -> Tensor: ...\n    def nelement(self) -> _int: ...\n    @overload\n    def new_empty(self, size: _size, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n    @overload\n    def new_empty(self, *size: _int, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n    def new_full(self, size: _size, fill_value: Number, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n    def new_ones(self, size: _size, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...\n    def new_tensor(self, data: Any, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...\n    @overload\n    def new_zeros(self, size: _size, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n    @overload\n    def new_zeros(self, *size: _int, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n    def normal_(self, mean: _float=0, std: _float=1, *, generator: Generator=None) -> Tensor: ...\n    def numel(self) -> _int: ...\n    def numpy(self) -> Any: ...\n    def orgqr(self, input2: Tensor) -> Tensor: ...\n    def ormqr(self, input2: Tensor, input3: Tensor, left: _bool=True, transpose: _bool=False) -> Tensor: ...\n    @overload\n    def permute(self, dims: _size) -> Tensor: ...\n    @overload\n    def permute(self, *dims: _int) -> Tensor: ...\n    def pin_memory(self) -> Tensor: ...\n    def pinverse(self, rcond: _float=1e-15) -> Tensor: ...\n    def polygamma(self, n: _int) -> Tensor: ...\n    def polygamma_(self, n: _int) -> Tensor: ...\n    @overload\n    def pow(self, exponent: Number) -> Tensor: ...\n    @overload\n    def pow(self, exponent: Tensor) -> Tensor: ...\n    @overload\n    def pow_(self, exponent: Number) -> Tensor: ...\n    @overload\n    def pow_(self, exponent: Tensor) -> Tensor: ...\n    def prelu(self, weight: Tensor) -> Tensor: ...\n    @overload\n    def prod(self, *, dtype: Optional[_dtype]=None) -> Tensor: ...\n    @overload\n    def prod(self, dim: _int, keepdim: _bool=False, *, dtype: Optional[_dtype]=None) -> Tensor: ...\n    @overload\n    def prod(self, dim: Union[str, None], keepdim: _bool=False, *, dtype: Optional[_dtype]=None) -> Tensor: ...\n    def put_(self, index: Tensor, source: Tensor, accumulate: _bool=False) -> Tensor: ...\n    def q_per_channel_axis(self) -> _int: ...\n    def q_per_channel_scales(self) -> Tensor: ...\n    def q_per_channel_zero_points(self) -> Tensor: ...\n    def q_scale(self) -> _float: ...\n    def q_zero_point(self) -> _int: ...\n    def qr(self, some: _bool=True) -> Tuple[Tensor, Tensor]: ...\n    def qscheme(self) -> _qscheme: ...\n    @overload\n    def random_(self, from_: _int, to: Optional[_int], *, generator: Generator=None) -> Tensor: ...\n    @overload\n    def random_(self, to: _int, *, generator: Generator=None) -> Tensor: ...\n    @overload\n    def random_(self, *, generator: Generator=None) -> Tensor: ...\n    def reciprocal(self) -> Tensor: ...\n    def reciprocal_(self) -> Tensor: ...\n    def refine_names(self, names: List[Union[str, None]]) -> Tensor: ...\n    def relu(self) -> Tensor: ...\n    def relu_(self) -> Tensor: ...\n    @overload\n    def remainder(self, other: Number) -> Tensor: ...\n    @overload\n    def remainder(self, other: Tensor) -> Tensor: ...\n    @overload\n    def remainder_(self, other: Number) -> Tensor: ...\n    @overload\n    def remainder_(self, other: Tensor) -> Tensor: ...\n    def rename(self, names: Optional[List[Union[str, None]]]) -> Tensor: ...\n    def rename_(self, names: Optional[List[Union[str, None]]]) -> Tensor: ...\n    def renorm(self, p: Number, dim: _int, maxnorm: Number) -> Tensor: ...\n    def renorm_(self, p: Number, dim: _int, maxnorm: Number) -> Tensor: ...\n    @overload\n    def repeat(self, repeats: _size) -> Tensor: ...\n    @overload\n    def repeat(self, *repeats: _int) -> Tensor: ...\n    @overload\n    def repeat_interleave(self, repeats: Tensor, dim: Optional[_int]=None) -> Tensor: ...\n    @overload\n    def repeat_interleave(self, repeats: _int, dim: Optional[_int]=None) -> Tensor: ...\n    def requires_grad_(self, mode: _bool=True) -> Tensor: ...\n    @overload\n    def reshape(self, shape: _size) -> Tensor: ...\n    @overload\n    def reshape(self, *shape: _int) -> Tensor: ...\n    def reshape_as(self, other: Tensor) -> Tensor: ...\n    @overload\n    def resize_(self, size: _size, *, memory_format: Optional[memory_format]=None) -> Tensor: ...\n    @overload\n    def resize_(self, *size: _int, memory_format: Optional[memory_format]=None) -> Tensor: ...\n    def resize_as_(self, the_template: Tensor, *, memory_format: Optional[memory_format]=None) -> Tensor: ...\n    def rfft(self, signal_ndim: _int, normalized: _bool=False, onesided: _bool=True) -> Tensor: ...\n    def roll(self, shifts: Union[_int, _size], dims: Union[_int, _size]=()) -> Tensor: ...\n    def rot90(self, k: _int=1, dims: _size=(0,1)) -> Tensor: ...\n    def round(self) -> Tensor: ...\n    def round_(self) -> Tensor: ...\n    def rsqrt(self) -> Tensor: ...\n    def rsqrt_(self) -> Tensor: ...\n    @overload\n    def scatter(self, dim: _int, index: Tensor, src: Tensor) -> Tensor: ...\n    @overload\n    def scatter(self, dim: _int, index: Tensor, value: Number) -> Tensor: ...\n    @overload\n    def scatter(self, dim: Union[str, None], index: Tensor, src: Tensor) -> Tensor: ...\n    @overload\n    def scatter(self, dim: Union[str, None], index: Tensor, value: Number) -> Tensor: ...\n    @overload\n    def scatter_(self, dim: _int, index: Tensor, src: Tensor) -> Tensor: ...\n    @overload\n    def scatter_(self, dim: _int, index: Tensor, value: Number) -> Tensor: ...\n    @overload\n    def scatter_add(self, dim: _int, index: Tensor, src: Tensor) -> Tensor: ...\n    @overload\n    def scatter_add(self, dim: Union[str, None], index: Tensor, src: Tensor) -> Tensor: ...\n    def scatter_add_(self, dim: _int, index: Tensor, src: Tensor) -> Tensor: ...\n    @overload\n    def select(self, dim: Union[str, None], index: _int) -> Tensor: ...\n    @overload\n    def select(self, dim: _int, index: _int) -> Tensor: ...\n    @overload\n    def set_(self, source: Storage) -> Tensor: ...\n    @overload\n    def set_(self, source: Storage, storage_offset: _int, size: _size, stride: _size=()) -> Tensor: ...\n    @overload\n    def set_(self, source: Tensor) -> Tensor: ...\n    @overload\n    def set_(self) -> Tensor: ...\n    def short(self) -> Tensor: ...\n    def sigmoid(self) -> Tensor: ...\n    def sigmoid_(self) -> Tensor: ...\n    def sign(self) -> Tensor: ...\n    def sign_(self) -> Tensor: ...\n    def sin(self) -> Tensor: ...\n    def sin_(self) -> Tensor: ...\n    def sinh(self) -> Tensor: ...\n    def sinh_(self) -> Tensor: ...\n    @overload\n    def size(self) -> Size: ...\n    @overload\n    def size(self, _int) -> _int: ...\n    def slogdet(self) -> Tuple[Tensor, Tensor]: ...\n    def smm(self, mat2: Tensor) -> Tensor: ...\n    @overload\n    def softmax(self, dim: _int, dtype: Optional[_dtype]=None) -> Tensor: ...\n    @overload\n    def softmax(self, dim: Union[str, None], *, dtype: Optional[_dtype]=None) -> Tensor: ...\n    def solve(self, A: Tensor) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def sort(self, dim: _int=-1, descending: _bool=False) -> Tuple[Tensor, Tensor]: ...\n    @overload\n    def sort(self, dim: Union[str, None], descending: _bool=False) -> Tuple[Tensor, Tensor]: ...\n    def sparse_dim(self) -> _int: ...\n    def sparse_mask(self, mask: Tensor) -> Tensor: ...\n    def sparse_resize_(self, size: _size, sparse_dim: _int, dense_dim: _int) -> Tensor: ...\n    def sparse_resize_and_clear_(self, size: _size, sparse_dim: _int, dense_dim: _int) -> Tensor: ...\n    def split_with_sizes(self, split_sizes: _size, dim: _int=0) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...\n    def sqrt(self) -> Tensor: ...\n    def sqrt_(self) -> Tensor: ...\n    def square(self) -> Tensor: ...\n    def square_(self) -> Tensor: ...\n    @overload\n    def squeeze(self) -> Tensor: ...\n    @overload\n    def squeeze(self, dim: _int) -> Tensor: ...\n    @overload\n    def squeeze(self, dim: Union[str, None]) -> Tensor: ...\n    @overload\n    def squeeze_(self) -> Tensor: ...\n    @overload\n    def squeeze_(self, dim: _int) -> Tensor: ...\n    @overload\n    def squeeze_(self, dim: Union[str, None]) -> Tensor: ...\n    def sspaddmm(self, mat1: Tensor, mat2: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...\n    @overload\n    def std(self, unbiased: _bool=True) -> Tensor: ...\n    @overload\n    def std(self, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False) -> Tensor: ...\n    @overload\n    def std(self, dim: List[Union[str, None]], unbiased: _bool=True, keepdim: _bool=False) -> Tensor: ...\n    def storage(self) -> Storage: ...\n    def storage_offset(self) -> _int: ...\n    @overload\n    def stride(self) -> Tuple[_int]: ...\n    @overload\n    def stride(self, _int) -> _int: ...\n    def sub(self, other: Union[Tensor, Number], *, alpha: Optional[Number]=1, out: Optional[Tensor]=None) -> Tensor: ...\n    def sub_(self, other: Union[Tensor, Number], *, alpha: Optional[Number]=1) -> Tensor: ...\n    @overload\n    def sum(self, *, dtype: Optional[_dtype]=None) -> Tensor: ...\n    @overload\n    def sum(self, dim: Union[_int, _size], keepdim: _bool=False, *, dtype: Optional[_dtype]=None) -> Tensor: ...\n    @overload\n    def sum(self, dim: List[Union[str, None]], keepdim: _bool=False, *, dtype: Optional[_dtype]=None) -> Tensor: ...\n    @overload\n    def sum_to_size(self, size: _size) -> Tensor: ...\n    @overload\n    def sum_to_size(self, *size: _int) -> Tensor: ...\n    def svd(self, some: _bool=True, compute_uv: _bool=True) -> Tuple[Tensor, Tensor, Tensor]: ...\n    def symeig(self, eigenvectors: _bool=False, upper: _bool=True) -> Tuple[Tensor, Tensor]: ...\n    def t(self) -> Tensor: ...\n    def t_(self) -> Tensor: ...\n    def take(self, index: Tensor) -> Tensor: ...\n    def tan(self) -> Tensor: ...\n    def tan_(self) -> Tensor: ...\n    def tanh(self) -> Tensor: ...\n    def tanh_(self) -> Tensor: ...\n    @overload\n    def to(self, dtype: _dtype, non_blocking: _bool=False, copy: _bool=False) -> Tensor: ...\n    @overload\n    def to(self, device: Optional[Union[_device, str]]=None, dtype: Optional[_dtype]=None, non_blocking: _bool=False, copy: _bool=False) -> Tensor: ...\n    @overload\n    def to(self, other: Tensor, non_blocking: _bool=False, copy: _bool=False) -> Tensor: ...\n    def to_dense(self) -> Tensor: ...\n    def to_mkldnn(self) -> Tensor: ...\n    @overload\n    def to_sparse(self, sparse_dim: _int) -> Tensor: ...\n    @overload\n    def to_sparse(self) -> Tensor: ...\n    def tolist(self) -> List: ...\n    def topk(self, k: _int, dim: _int=-1, largest: _bool=True, sorted: _bool=True) -> Tuple[Tensor, Tensor]: ...\n    def trace(self) -> Tensor: ...\n    @overload\n    def transpose(self, dim0: _int, dim1: _int) -> Tensor: ...\n    @overload\n    def transpose(self, dim0: Union[str, None], dim1: Union[str, None]) -> Tensor: ...\n    def transpose_(self, dim0: _int, dim1: _int) -> Tensor: ...\n    def triangular_solve(self, A: Tensor, upper: _bool=True, transpose: _bool=False, unitriangular: _bool=False) -> Tuple[Tensor, Tensor]: ...\n    def tril(self, diagonal: _int=0) -> Tensor: ...\n    def tril_(self, diagonal: _int=0) -> Tensor: ...\n    def triu(self, diagonal: _int=0) -> Tensor: ...\n    def triu_(self, diagonal: _int=0) -> Tensor: ...\n    def true_divide(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    def true_divide_(self, other: Union[Tensor, Number]) -> Tensor: ...\n    def trunc(self) -> Tensor: ...\n    def trunc_(self) -> Tensor: ...\n    @overload\n    def type(self, dtype: None=None, non_blocking: _bool=False) -> str: ...\n    @overload\n    def type(self, dtype: Union[str, _dtype], non_blocking: _bool=False) -> Tensor: ...\n    def type_as(self, other: Tensor) -> Tensor: ...\n    @overload\n    def unbind(self, dim: _int=0) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...\n    @overload\n    def unbind(self, dim: Union[str, None]) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...\n    @overload\n    def unflatten(self, dim: Union[str, None], sizes: _size, names: List[Union[str, None]]) -> Tensor: ...\n    @overload\n    def unflatten(self, dim: _int, sizes: _size, names: List[Union[str, None]]) -> Tensor: ...\n    def unfold(self, dimension: _int, size: _int, step: _int) -> Tensor: ...\n    def uniform_(self, from_: _float=0, to: _float=1, *, generator: Generator=None) -> Tensor: ...\n    def unsqueeze(self, dim: _int) -> Tensor: ...\n    def unsqueeze_(self, dim: _int) -> Tensor: ...\n    def values(self) -> Tensor: ...\n    @overload\n    def var(self, unbiased: _bool=True) -> Tensor: ...\n    @overload\n    def var(self, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False) -> Tensor: ...\n    @overload\n    def var(self, dim: List[Union[str, None]], unbiased: _bool=True, keepdim: _bool=False) -> Tensor: ...\n    @overload\n    def view(self, size: _size) -> Tensor: ...\n    @overload\n    def view(self, *size: _int) -> Tensor: ...\n    def view_as(self, other: Tensor) -> Tensor: ...\n    def where(self, condition: Tensor, other: Tensor) -> Tensor: ...\n    def zero_(self) -> Tensor: ...\n\n    # Manually defined methods from torch/tensor.py\n    def __len__(self) -> _int: ...\n    def __iter__(self) -> Iterator[Tensor]: ...\n    def __contains__(self, item: Union[Tensor, Number]) -> _bool: ...\n    def register_hook(self, hook: Callable) -> Any: ...\n    def retain_grad(self) -> None: ...\n    def is_shared(self) -> _bool: ...\n    def share_memory_(self) -> None: ...\n    # TODO: fill in the types for these, or otherwise figure out some\n    # way to not have to write these out again...\n    def nonzero(self, *, as_tuple=True): ...\n    def norm(self, p=\"fro\", dim=None, keepdim=False): ...\n    def stft(self, n_fft, hop_length=None, win_length=None, window=None,\n             center=True, pad_mode='reflect', normalized=False, onesided=True): ...\n    def split(self, split_size, dim=0): ...\n    def unique(self, sorted=True, return_inverse=False, dim=None): ...\n    def unique_consecutive(self, sorted=True, return_inverse=False, return_counts=False, dim=None): ...\n    def lu(self, pivot=True, get_infos=False): ...\n\n@overload\ndef __and__(self: Tensor, other: Number) -> Tensor: ...\n@overload\ndef __and__(self: Tensor, other: Tensor) -> Tensor: ...\n@overload\ndef __lshift__(self: Tensor, other: Number) -> Tensor: ...\n@overload\ndef __lshift__(self: Tensor, other: Tensor) -> Tensor: ...\n@overload\ndef __or__(self: Tensor, other: Number) -> Tensor: ...\n@overload\ndef __or__(self: Tensor, other: Tensor) -> Tensor: ...\n@overload\ndef __rshift__(self: Tensor, other: Number) -> Tensor: ...\n@overload\ndef __rshift__(self: Tensor, other: Tensor) -> Tensor: ...\n@overload\ndef __xor__(self: Tensor, other: Number) -> Tensor: ...\n@overload\ndef __xor__(self: Tensor, other: Tensor) -> Tensor: ...\ndef _adaptive_avg_pool2d(self: Tensor, output_size: Union[_int, _size]) -> Tensor: ...\ndef _addr(self: Tensor, vec1: Tensor, vec2: Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[Tensor]=None) -> Tensor: ...\ndef _addr_(self: Tensor, vec1: Tensor, vec2: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...\ndef _amp_non_finite_check_and_unscale_(self: Tensor, found_inf: Tensor, inv_scale: Tensor) -> None: ...\ndef _amp_update_scale(growth_tracker: Tensor, current_scale: Tensor, found_inf: Tensor, scale_growth_factor: _float, scale_backoff_factor: _float, growth_interval: _int) -> Tensor: ...\ndef _baddbmm_mkl_(self: Tensor, batch1: Tensor, batch2: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...\ndef _batch_norm_impl_index(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: _bool, momentum: _float, eps: _float, cudnn_enabled: _bool) -> Tuple[Tensor, Tensor, Tensor, Tensor, _int]: ...\ndef _cast_Byte(self: Tensor, non_blocking: _bool=False) -> Tensor: ...\ndef _cast_Char(self: Tensor, non_blocking: _bool=False) -> Tensor: ...\ndef _cast_Double(self: Tensor, non_blocking: _bool=False) -> Tensor: ...\ndef _cast_Float(self: Tensor, non_blocking: _bool=False) -> Tensor: ...\ndef _cast_Half(self: Tensor, non_blocking: _bool=False) -> Tensor: ...\ndef _cast_Int(self: Tensor, non_blocking: _bool=False) -> Tensor: ...\ndef _cast_Long(self: Tensor, non_blocking: _bool=False) -> Tensor: ...\ndef _cast_Short(self: Tensor, non_blocking: _bool=False) -> Tensor: ...\ndef _cat(tensors: Union[Tuple[Tensor, ...], List[Tensor]], dim: _int=0, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef _convolution(input: Tensor, weight: Tensor, bias: Optional[Tensor], stride: _size, padding: _size, dilation: _size, transposed: _bool, output_padding: _size, groups: _int, benchmark: _bool, deterministic: _bool, cudnn_enabled: _bool) -> Tensor: ...\ndef _convolution_nogroup(input: Tensor, weight: Tensor, bias: Optional[Tensor], stride: _size, padding: _size, dilation: _size, transposed: _bool, output_padding: _size) -> Tensor: ...\ndef _copy_from(self: Tensor, dst: Tensor, non_blocking: _bool=False) -> Tensor: ...\ndef _ctc_loss(log_probs: Tensor, targets: Tensor, input_lengths: _size, target_lengths: _size, blank: _int=0, zero_infinity: _bool=False) -> Tuple[Tensor, Tensor]: ...\ndef _cudnn_ctc_loss(log_probs: Tensor, targets: Tensor, input_lengths: _size, target_lengths: _size, blank: _int, deterministic: _bool, zero_infinity: _bool) -> Tuple[Tensor, Tensor]: ...\ndef _cudnn_init_dropout_state(dropout: _float, train: _bool, dropout_seed: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef _cudnn_rnn(input: Tensor, weight: Union[Tuple[Tensor, ...], List[Tensor]], weight_stride0: _int, weight_buf: Optional[Tensor], hx: Tensor, cx: Optional[Tensor], mode: _int, hidden_size: _int, num_layers: _int, batch_first: _bool, dropout: _float, train: _bool, bidirectional: _bool, batch_sizes: _size, dropout_state: Optional[Tensor]) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]: ...\ndef _cudnn_rnn_flatten_weight(weight_arr: Union[Tuple[Tensor, ...], List[Tensor]], weight_stride0: _int, input_size: _int, mode: _int, hidden_size: _int, num_layers: _int, batch_first: _bool, bidirectional: _bool) -> Tensor: ...\ndef _cufft_clear_plan_cache(device_index: _int) -> None: ...\ndef _cufft_get_plan_cache_max_size(device_index: _int) -> _int: ...\ndef _cufft_get_plan_cache_size(device_index: _int) -> _int: ...\ndef _cufft_set_plan_cache_max_size(device_index: _int, max_size: _int) -> None: ...\ndef _cummax_helper(self: Tensor, values: Tensor, indices: Tensor, dim: _int) -> None: ...\ndef _cummin_helper(self: Tensor, values: Tensor, indices: Tensor, dim: _int) -> None: ...\ndef _debug_has_internal_overlap(self: Tensor) -> _int: ...\ndef _dim_arange(like: Tensor, dim: _int) -> Tensor: ...\ndef _dirichlet_grad(x: Tensor, alpha: Tensor, total: Tensor) -> Tensor: ...\ndef _embedding_bag(weight: Tensor, indices: Tensor, offsets: Tensor, scale_grad_by_freq: _bool=False, mode: _int=0, sparse: _bool=False, per_sample_weights: Optional[Tensor]=None, include_last_offset: _bool=False) -> Tuple[Tensor, Tensor, Tensor, Tensor]: ...\n@overload\ndef _empty_affine_quantized(size: _size, *, scale: _float=1, zero_point: _int=0, memory_format: Optional[memory_format]=contiguous_format, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef _empty_affine_quantized(*size: _int, scale: _float=1, zero_point: _int=0, memory_format: Optional[memory_format]=contiguous_format, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef _empty_per_channel_affine_quantized(size: _size, *, scales: Tensor, zero_points: Tensor, axis: _int, memory_format: Optional[memory_format]=contiguous_format, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef _empty_per_channel_affine_quantized(*size: _int, scales: Tensor, zero_points: Tensor, axis: _int, memory_format: Optional[memory_format]=contiguous_format, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef _fft_with_size(self: Tensor, signal_ndim: _int, complex_input: _bool, complex_output: _bool, inverse: _bool, checked_signal_sizes: _size, normalized: _bool, onesided: _bool, output_sizes: _size) -> Tensor: ...\ndef _fused_dropout(self: Tensor, p: _float, generator: Generator=None) -> Tuple[Tensor, Tensor]: ...\ndef _has_compatible_shallow_copy_type(self: Tensor, from_: Tensor) -> _bool: ...\ndef _index_copy_(self: Tensor, dim: _int, index: Tensor, source: Tensor) -> Tensor: ...\ndef _index_put_impl_(self: Tensor, indices: Optional[Union[Tuple[Tensor, ...], List[Tensor]]], values: Tensor, accumulate: _bool=False, unsafe: _bool=False) -> Tensor: ...\ndef _log_softmax(self: Tensor, dim: _int, half_to_float: _bool) -> Tensor: ...\ndef _log_softmax_backward_data(grad_output: Tensor, output: Tensor, dim: _int, self: Tensor) -> Tensor: ...\ndef _lu_solve_helper(self: Tensor, LU_data: Tensor, LU_pivots: Tensor) -> Tensor: ...\ndef _lu_with_info(self: Tensor, pivot: _bool=True, check_errors: _bool=True) -> Tuple[Tensor, Tensor, Tensor]: ...\ndef _make_per_channel_quantized_tensor(self: Tensor, scale: Tensor, zero_point: Tensor, axis: _int) -> Tensor: ...\ndef _make_per_tensor_quantized_tensor(self: Tensor, scale: _float, zero_point: _int) -> Tensor: ...\ndef _masked_scale(self: Tensor, mask: Tensor, scale: _float) -> Tensor: ...\ndef _max(self: Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\ndef _min(self: Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\ndef _mkldnn_reshape(self: Tensor, shape: _size) -> Tensor: ...\ndef _mkldnn_transpose(self: Tensor, dim0: _int, dim1: _int) -> Tensor: ...\ndef _mkldnn_transpose_(self: Tensor, dim0: _int, dim1: _int) -> Tensor: ...\ndef _mode(self: Tensor, dim: _int=-1, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\ndef _multinomial_alias_draw(J: Tensor, q: Tensor, num_samples: _int, *, generator: Generator=None) -> Tensor: ...\ndef _multinomial_alias_setup(probs: Tensor) -> Tuple[Tensor, Tensor]: ...\ndef _nnpack_available() -> _bool: ...\ndef _nnpack_spatial_convolution(input: Tensor, weight: Tensor, bias: Optional[Tensor], padding: Union[_int, _size], stride: Union[_int, _size]=1) -> Tensor: ...\ndef _pack_padded_sequence(input: Tensor, lengths: Tensor, batch_first: _bool) -> Tuple[Tensor, Tensor]: ...\ndef _pad_packed_sequence(data: Tensor, batch_sizes: Tensor, batch_first: _bool, padding_value: Number, total_length: _int) -> Tuple[Tensor, Tensor]: ...\ndef _reshape_from_tensor(self: Tensor, shape: Tensor) -> Tensor: ...\ndef _s_where(condition: Tensor, self: Tensor, other: Tensor) -> Tensor: ...\ndef _sample_dirichlet(self: Tensor, generator: Generator=None) -> Tensor: ...\ndef _shape_as_tensor(self: Tensor) -> Tensor: ...\ndef _sobol_engine_draw(quasi: Tensor, n: _int, sobolstate: Tensor, dimension: _int, num_generated: _int, dtype: Optional[_dtype]) -> Tuple[Tensor, Tensor]: ...\ndef _sobol_engine_ff_(self: Tensor, n: _int, sobolstate: Tensor, dimension: _int, num_generated: _int) -> Tensor: ...\ndef _sobol_engine_initialize_state_(self: Tensor, dimension: _int) -> Tensor: ...\ndef _sobol_engine_scramble_(self: Tensor, ltm: Tensor, dimension: _int) -> Tensor: ...\ndef _softmax(self: Tensor, dim: _int, half_to_float: _bool) -> Tensor: ...\ndef _softmax_backward_data(grad_output: Tensor, output: Tensor, dim: _int, self: Tensor) -> Tensor: ...\ndef _sparse_addmm(self: Tensor, sparse: Tensor, dense: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...\ndef _sparse_mm(sparse: Tensor, dense: Tensor) -> Tensor: ...\n@overload\ndef _sparse_sum(self: Tensor) -> Tensor: ...\n@overload\ndef _sparse_sum(self: Tensor, *, dtype: _dtype) -> Tensor: ...\n@overload\ndef _sparse_sum(self: Tensor, dim: Union[_int, _size]) -> Tensor: ...\n@overload\ndef _sparse_sum(self: Tensor, dim: Union[_int, _size], *, dtype: _dtype) -> Tensor: ...\ndef _standard_gamma(self: Tensor, generator: Generator=None) -> Tensor: ...\ndef _standard_gamma_grad(self: Tensor, output: Tensor) -> Tensor: ...\ndef _std(self: Tensor, unbiased: _bool=True) -> Tensor: ...\ndef _trilinear(i1: Tensor, i2: Tensor, i3: Tensor, expand1: _size, expand2: _size, expand3: _size, sumdim: _size, unroll_dim: _int=1) -> Tensor: ...\ndef _unique(self: Tensor, sorted: _bool=True, return_inverse: _bool=False) -> Tuple[Tensor, Tensor]: ...\ndef _unique2(self: Tensor, sorted: _bool=True, return_inverse: _bool=False, return_counts: _bool=False) -> Tuple[Tensor, Tensor, Tensor]: ...\ndef _use_cudnn_ctc_loss(log_probs: Tensor, targets: Tensor, input_lengths: _size, target_lengths: _size, blank: _int) -> _bool: ...\ndef _use_cudnn_rnn_flatten_weight() -> _bool: ...\ndef _var(self: Tensor, unbiased: _bool=True) -> Tensor: ...\ndef _weight_norm(v: Tensor, g: Tensor, dim: _int=0) -> Tensor: ...\ndef _weight_norm_cuda_interface(v: Tensor, g: Tensor, dim: _int=0) -> Tuple[Tensor, Tensor]: ...\ndef abs(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef abs_(self: Tensor) -> Tensor: ...\ndef acos(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef acos_(self: Tensor) -> Tensor: ...\ndef adaptive_avg_pool1d(self: Tensor, output_size: Union[_int, _size]) -> Tensor: ...\ndef adaptive_max_pool1d(self: Tensor, output_size: Union[_int, _size]) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef add(input: Union[Tensor, Number], other: Union[Tensor, Number], *, alpha: Optional[Number]=1, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef add(self: Tensor, alpha: Number, other: Tensor) -> Tensor: ...\n@overload\ndef add(self: Tensor, alpha: Number, other: Tensor, *, out: Tensor) -> Tensor: ...\n@overload\ndef addbmm(self: Tensor, batch1: Tensor, batch2: Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef addbmm(beta: Number, self: Tensor, alpha: Number, batch1: Tensor, batch2: Tensor) -> Tensor: ...\n@overload\ndef addbmm(beta: Number, self: Tensor, alpha: Number, batch1: Tensor, batch2: Tensor, *, out: Tensor) -> Tensor: ...\n@overload\ndef addbmm(beta: Number, self: Tensor, batch1: Tensor, batch2: Tensor) -> Tensor: ...\n@overload\ndef addbmm(beta: Number, self: Tensor, batch1: Tensor, batch2: Tensor, *, out: Tensor) -> Tensor: ...\n@overload\ndef addcdiv(self: Tensor, tensor1: Tensor, tensor2: Tensor, *, value: Number=1, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef addcdiv(self: Tensor, value: Number, tensor1: Tensor, tensor2: Tensor) -> Tensor: ...\n@overload\ndef addcdiv(self: Tensor, value: Number, tensor1: Tensor, tensor2: Tensor, *, out: Tensor) -> Tensor: ...\n@overload\ndef addcmul(self: Tensor, tensor1: Tensor, tensor2: Tensor, *, value: Number=1, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef addcmul(self: Tensor, value: Number, tensor1: Tensor, tensor2: Tensor) -> Tensor: ...\n@overload\ndef addcmul(self: Tensor, value: Number, tensor1: Tensor, tensor2: Tensor, *, out: Tensor) -> Tensor: ...\n@overload\ndef addmm(self: Tensor, mat1: Tensor, mat2: Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef addmm(beta: Number, self: Tensor, alpha: Number, mat1: Tensor, mat2: Tensor) -> Tensor: ...\n@overload\ndef addmm(beta: Number, self: Tensor, alpha: Number, mat1: Tensor, mat2: Tensor, *, out: Tensor) -> Tensor: ...\n@overload\ndef addmm(beta: Number, self: Tensor, mat1: Tensor, mat2: Tensor) -> Tensor: ...\n@overload\ndef addmm(beta: Number, self: Tensor, mat1: Tensor, mat2: Tensor, *, out: Tensor) -> Tensor: ...\n@overload\ndef addmv(self: Tensor, mat: Tensor, vec: Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef addmv(beta: Number, self: Tensor, alpha: Number, mat: Tensor, vec: Tensor) -> Tensor: ...\n@overload\ndef addmv(beta: Number, self: Tensor, alpha: Number, mat: Tensor, vec: Tensor, *, out: Tensor) -> Tensor: ...\n@overload\ndef addmv(beta: Number, self: Tensor, mat: Tensor, vec: Tensor) -> Tensor: ...\n@overload\ndef addmv(beta: Number, self: Tensor, mat: Tensor, vec: Tensor, *, out: Tensor) -> Tensor: ...\ndef addmv_(self: Tensor, mat: Tensor, vec: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...\n@overload\ndef addr(self: Tensor, vec1: Tensor, vec2: Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef addr(beta: Number, self: Tensor, alpha: Number, vec1: Tensor, vec2: Tensor) -> Tensor: ...\n@overload\ndef addr(beta: Number, self: Tensor, alpha: Number, vec1: Tensor, vec2: Tensor, *, out: Tensor) -> Tensor: ...\n@overload\ndef addr(beta: Number, self: Tensor, vec1: Tensor, vec2: Tensor) -> Tensor: ...\n@overload\ndef addr(beta: Number, self: Tensor, vec1: Tensor, vec2: Tensor, *, out: Tensor) -> Tensor: ...\ndef affine_grid_generator(theta: Tensor, size: _size, align_corners: _bool) -> Tensor: ...\n@overload\ndef all(self: Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef all(self: Tensor, dim: Union[str, None], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef all(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef allclose(self: Tensor, other: Tensor, rtol: _float=1e-05, atol: _float=1e-08, equal_nan: _bool=False) -> _bool: ...\ndef alpha_dropout(input: Tensor, p: _float, train: _bool) -> Tensor: ...\ndef alpha_dropout_(self: Tensor, p: _float, train: _bool) -> Tensor: ...\ndef angle(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef any(self: Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef any(self: Tensor, dim: Union[str, None], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef any(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef arange(start: Number, end: Number, step: Number, *, out: Optional[Tensor]=None, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...\n@overload\ndef arange(start: Number, end: Number, *, out: Optional[Tensor]=None, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...\n@overload\ndef arange(end: Number, *, out: Optional[Tensor]=None, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...\ndef argmax(self: Tensor, dim: Optional[_int]=None, keepdim: _bool=False) -> Tensor: ...\ndef argmin(self: Tensor, dim: Optional[_int]=None, keepdim: _bool=False) -> Tensor: ...\n@overload\ndef argsort(self: Tensor, dim: _int=-1, descending: _bool=False) -> Tensor: ...\n@overload\ndef argsort(self: Tensor, dim: Union[str, None], descending: _bool=False) -> Tensor: ...\ndef as_strided(self: Tensor, size: _size, stride: _size, storage_offset: Optional[_int]=None) -> Tensor: ...\ndef as_strided_(self: Tensor, size: _size, stride: _size, storage_offset: Optional[_int]=None) -> Tensor: ...\ndef as_tensor(data: Any, dtype: _dtype=None, device: Optional[_device]=None) -> Tensor: ...\ndef asin(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef asin_(self: Tensor) -> Tensor: ...\ndef atan(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef atan2(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef atan_(self: Tensor) -> Tensor: ...\ndef avg_pool1d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, ceil_mode: _bool=False, count_include_pad: _bool=True) -> Tensor: ...\n@overload\ndef baddbmm(self: Tensor, batch1: Tensor, batch2: Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef baddbmm(beta: Number, self: Tensor, alpha: Number, batch1: Tensor, batch2: Tensor) -> Tensor: ...\n@overload\ndef baddbmm(beta: Number, self: Tensor, alpha: Number, batch1: Tensor, batch2: Tensor, *, out: Tensor) -> Tensor: ...\n@overload\ndef baddbmm(beta: Number, self: Tensor, batch1: Tensor, batch2: Tensor) -> Tensor: ...\n@overload\ndef baddbmm(beta: Number, self: Tensor, batch1: Tensor, batch2: Tensor, *, out: Tensor) -> Tensor: ...\n@overload\ndef bartlett_window(window_length: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef bartlett_window(window_length: _int, periodic: _bool, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef batch_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: _bool, momentum: _float, eps: _float, cudnn_enabled: _bool) -> Tensor: ...\ndef batch_norm_backward_elemt(grad_out: Tensor, input: Tensor, mean: Tensor, invstd: Tensor, weight: Optional[Tensor], mean_dy: Tensor, mean_dy_xmu: Tensor) -> Tensor: ...\ndef batch_norm_backward_reduce(grad_out: Tensor, input: Tensor, mean: Tensor, invstd: Tensor, weight: Optional[Tensor], input_g: _bool, weight_g: _bool, bias_g: _bool) -> Tuple[Tensor, Tensor, Tensor, Tensor]: ...\ndef batch_norm_elemt(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], mean: Tensor, invstd: Tensor, eps: _float, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef batch_norm_gather_stats(input: Tensor, mean: Tensor, invstd: Tensor, running_mean: Optional[Tensor], running_var: Optional[Tensor], momentum: _float, eps: _float, count: _int) -> Tuple[Tensor, Tensor]: ...\ndef batch_norm_gather_stats_with_counts(input: Tensor, mean: Tensor, invstd: Tensor, running_mean: Optional[Tensor], running_var: Optional[Tensor], momentum: _float, eps: _float, counts: _size) -> Tuple[Tensor, Tensor]: ...\ndef batch_norm_stats(input: Tensor, eps: _float) -> Tuple[Tensor, Tensor]: ...\ndef batch_norm_update_stats(input: Tensor, running_mean: Optional[Tensor], running_var: Optional[Tensor], momentum: _float) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef bernoulli(self: Tensor, *, generator: Generator=None, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef bernoulli(self: Tensor, p: _float, *, generator: Generator=None, out: Optional[Tensor]=None) -> Tensor: ...\ndef bilinear(input1: Tensor, input2: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor: ...\ndef bincount(self: Tensor, weights: Optional[Tensor]=None, minlength: _int=0) -> Tensor: ...\n@overload\ndef bitwise_and(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef bitwise_and(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef bitwise_not(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef bitwise_or(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef bitwise_or(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef bitwise_xor(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef bitwise_xor(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef blackman_window(window_length: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef blackman_window(window_length: _int, periodic: _bool, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef bmm(self: Tensor, mat2: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef can_cast(from_: _dtype, to: _dtype) -> _bool: ...\n@overload\ndef cat(tensors: Union[Tuple[Tensor, ...], List[Tensor]], dim: _int=0, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef cat(tensors: Union[Tuple[Tensor, ...], List[Tensor]], dim: Union[str, None], *, out: Optional[Tensor]=None) -> Tensor: ...\ndef ceil(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef ceil_(self: Tensor) -> Tensor: ...\ndef celu(self: Tensor, alpha: Number=1.0) -> Tensor: ...\ndef celu_(self: Tensor, alpha: Number=1.0) -> Tensor: ...\ndef cholesky(self: Tensor, upper: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef cholesky_inverse(self: Tensor, upper: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef cholesky_solve(self: Tensor, input2: Tensor, upper: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef chunk(self: Tensor, chunks: _int, dim: _int=0) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...\ndef clamp(self, min: _float=-inf, max: _float=inf, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef clamp_max(self: Tensor, max: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef clamp_max_(self: Tensor, max: Number) -> Tensor: ...\ndef clamp_min(self: Tensor, min: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef clamp_min_(self: Tensor, min: Number) -> Tensor: ...\ndef clone(self: Tensor, *, memory_format: Optional[memory_format]=None) -> Tensor: ...\ndef combinations(self: Tensor, r: _int=2, with_replacement: _bool=False) -> Tensor: ...\ndef conj(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef constant_pad_nd(self: Tensor, pad: _size, value: Number=0) -> Tensor: ...\ndef conv1d(input: Tensor, weight: Tensor, bias: Optional[Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, groups: _int=1) -> Tensor: ...\ndef conv2d(input: Tensor, weight: Tensor, bias: Optional[Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, groups: _int=1) -> Tensor: ...\ndef conv3d(input: Tensor, weight: Tensor, bias: Optional[Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, groups: _int=1) -> Tensor: ...\ndef conv_tbc(self: Tensor, weight: Tensor, bias: Tensor, pad: _int=0) -> Tensor: ...\ndef conv_transpose1d(input: Tensor, weight: Tensor, bias: Optional[Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, output_padding: Union[_int, _size]=0, groups: _int=1, dilation: Union[_int, _size]=1) -> Tensor: ...\ndef conv_transpose2d(input: Tensor, weight: Tensor, bias: Optional[Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, output_padding: Union[_int, _size]=0, groups: _int=1, dilation: Union[_int, _size]=1) -> Tensor: ...\ndef conv_transpose3d(input: Tensor, weight: Tensor, bias: Optional[Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, output_padding: Union[_int, _size]=0, groups: _int=1, dilation: Union[_int, _size]=1) -> Tensor: ...\ndef convolution(input: Tensor, weight: Tensor, bias: Optional[Tensor], stride: _size, padding: _size, dilation: _size, transposed: _bool, output_padding: _size, groups: _int) -> Tensor: ...\ndef cos(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef cos_(self: Tensor) -> Tensor: ...\ndef cosh(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef cosh_(self: Tensor) -> Tensor: ...\ndef cosine_similarity(x1: Tensor, x2: Tensor, dim: _int=1, eps: _float=1e-08) -> Tensor: ...\ndef cross(self: Tensor, other: Tensor, dim: Optional[_int]=None, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef cudnn_affine_grid_generator(theta: Tensor, N: _int, C: _int, H: _int, W: _int) -> Tensor: ...\ndef cudnn_batch_norm(input: Tensor, weight: Tensor, bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: _bool, exponential_average_factor: _float, epsilon: _float) -> Tuple[Tensor, Tensor, Tensor, Tensor]: ...\n@overload\ndef cudnn_convolution(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...\n@overload\ndef cudnn_convolution(self: Tensor, weight: Tensor, padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...\n@overload\ndef cudnn_convolution_transpose(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, output_padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...\n@overload\ndef cudnn_convolution_transpose(self: Tensor, weight: Tensor, padding: _size, output_padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...\ndef cudnn_grid_sampler(self: Tensor, grid: Tensor) -> Tensor: ...\ndef cudnn_is_acceptable(self: Tensor) -> _bool: ...\n@overload\ndef cummax(self: Tensor, dim: _int, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef cummax(self: Tensor, dim: Union[str, None], *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef cummin(self: Tensor, dim: _int, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef cummin(self: Tensor, dim: Union[str, None], *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef cumprod(self: Tensor, dim: _int, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef cumprod(self: Tensor, dim: Union[str, None], *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef cumsum(self: Tensor, dim: _int, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef cumsum(self: Tensor, dim: Union[str, None], *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...\ndef dequantize(self: Tensor) -> Tensor: ...\ndef det(self: Tensor) -> Tensor: ...\ndef detach(self: Tensor) -> Tensor: ...\ndef detach_(self: Tensor) -> Tensor: ...\ndef diag(self: Tensor, diagonal: _int=0, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef diag_embed(self: Tensor, offset: _int=0, dim1: _int=-2, dim2: _int=-1) -> Tensor: ...\ndef diagflat(self: Tensor, offset: _int=0) -> Tensor: ...\n@overload\ndef diagonal(self: Tensor, offset: _int=0, dim1: _int=0, dim2: _int=1) -> Tensor: ...\n@overload\ndef diagonal(self: Tensor, *, outdim: Union[str, None], dim1: Union[str, None], dim2: Union[str, None], offset: _int=0) -> Tensor: ...\ndef digamma(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef dist(self: Tensor, other: Tensor, p: Number=2) -> Tensor: ...\ndef div(input: Union[Tensor, Number], other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\ndef dot(self: Tensor, tensor: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef dropout(input: Tensor, p: _float, train: _bool) -> Tensor: ...\ndef dropout_(self: Tensor, p: _float, train: _bool) -> Tensor: ...\ndef eig(self: Tensor, eigenvectors: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\ndef embedding(weight: Tensor, indices: Tensor, padding_idx: _int=-1, scale_grad_by_freq: _bool=False, sparse: _bool=False) -> Tensor: ...\ndef embedding_bag(weight: Tensor, indices: Tensor, offsets: Tensor, scale_grad_by_freq: _bool=False, mode: _int=0, sparse: _bool=False, per_sample_weights: Optional[Tensor]=None, include_last_offset: _bool=False) -> Tuple[Tensor, Tensor, Tensor, Tensor]: ...\ndef embedding_renorm_(self: Tensor, indices: Tensor, max_norm: _float, norm_type: _float) -> Tensor: ...\n@overload\ndef empty(size: _size, *, names: Optional[List[Union[str, None]]], memory_format: Optional[memory_format]=None, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef empty(*size: _int, names: Optional[List[Union[str, None]]], memory_format: Optional[memory_format]=None, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef empty(size: _size, *, memory_format: Optional[memory_format]=None, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef empty(*size: _int, memory_format: Optional[memory_format]=None, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef empty_like(self: Tensor, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef empty_strided(size: _size, stride: _size, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef eq(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef eq(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef equal(self: Tensor, other: Tensor) -> _bool: ...\ndef erf(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef erf_(self: Tensor) -> Tensor: ...\ndef erfc(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef erfc_(self: Tensor) -> Tensor: ...\ndef erfinv(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef exp(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef exp_(self: Tensor) -> Tensor: ...\ndef expm1(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef expm1_(self: Tensor) -> Tensor: ...\n@overload\ndef eye(n: _int, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef eye(n: _int, m: _int, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef fake_quantize_per_channel_affine(self: Tensor, scale: Tensor, zero_point: Tensor, axis: _int, quant_min: _int, quant_max: _int) -> Tensor: ...\ndef fake_quantize_per_tensor_affine(self: Tensor, scale: _float, zero_point: _int, quant_min: _int, quant_max: _int) -> Tensor: ...\ndef fbgemm_linear_fp16_weight(input: Tensor, packed_weight: Tensor, bias: Tensor) -> Tensor: ...\ndef fbgemm_linear_fp16_weight_fp32_activation(input: Tensor, packed_weight: Tensor, bias: Tensor) -> Tensor: ...\ndef fbgemm_linear_int8_weight(input: Tensor, weight: Tensor, packed: Tensor, col_offsets: Tensor, weight_scale: Number, weight_zero_point: Number, bias: Tensor) -> Tensor: ...\ndef fbgemm_linear_int8_weight_fp32_activation(input: Tensor, weight: Tensor, packed: Tensor, col_offsets: Tensor, weight_scale: Number, weight_zero_point: Number, bias: Tensor) -> Tensor: ...\ndef fbgemm_linear_quantize_weight(input: Tensor) -> Tuple[Tensor, Tensor, _float, _int]: ...\ndef fbgemm_pack_gemm_matrix_fp16(input: Tensor) -> Tensor: ...\n@overload\ndef fbgemm_pack_quantized_matrix(input: Tensor) -> Tensor: ...\n@overload\ndef fbgemm_pack_quantized_matrix(input: Tensor, K: _int, N: _int) -> Tensor: ...\ndef feature_alpha_dropout(input: Tensor, p: _float, train: _bool) -> Tensor: ...\ndef feature_alpha_dropout_(self: Tensor, p: _float, train: _bool) -> Tensor: ...\ndef feature_dropout(input: Tensor, p: _float, train: _bool) -> Tensor: ...\ndef feature_dropout_(self: Tensor, p: _float, train: _bool) -> Tensor: ...\ndef fft(self: Tensor, signal_ndim: _int, normalized: _bool=False) -> Tensor: ...\n@overload\ndef fill_(self: Tensor, value: Number) -> Tensor: ...\n@overload\ndef fill_(self: Tensor, value: Tensor) -> Tensor: ...\n@overload\ndef flatten(self: Tensor, start_dim: _int=0, end_dim: _int=-1) -> Tensor: ...\n@overload\ndef flatten(self: Tensor, start_dim: _int, end_dim: _int, out_dim: Union[str, None]) -> Tensor: ...\n@overload\ndef flatten(self: Tensor, start_dim: Union[str, None], end_dim: Union[str, None], out_dim: Union[str, None]) -> Tensor: ...\n@overload\ndef flatten(self: Tensor, dims: List[Union[str, None]], out_dim: Union[str, None]) -> Tensor: ...\ndef flip(self: Tensor, dims: _size) -> Tensor: ...\ndef floor(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef floor_(self: Tensor) -> Tensor: ...\ndef floor_divide(input: Union[Tensor, Number], other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef fmod(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef fmod(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef frac(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef frac_(self: Tensor) -> Tensor: ...\n@overload\ndef frobenius_norm(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef frobenius_norm(self: Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef from_file(filename: str, shared: Optional[_bool]=None, size: Optional[_int]=0, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef from_numpy(ndarray) -> Tensor: ...\n@overload\ndef full(size: _size, fill_value: Number, *, out: Optional[Tensor]=None, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...\n@overload\ndef full(size: _size, fill_value: Number, *, names: List[Union[str, None]], dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...\ndef full_like(self: Tensor, fill_value: Number, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef gather(self: Tensor, dim: _int, index: Tensor, *, sparse_grad: _bool=False, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef gather(self: Tensor, dim: Union[str, None], index: Tensor, *, sparse_grad: _bool=False, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef ge(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef ge(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef geqrf(self: Tensor, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\ndef ger(self: Tensor, vec2: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef get_default_dtype() -> _dtype: ...\ndef get_num_interop_threads() -> _int: ...\ndef get_num_threads() -> _int: ...\ndef grid_sampler(input: Tensor, grid: Tensor, interpolation_mode: _int, padding_mode: _int, align_corners: _bool) -> Tensor: ...\ndef grid_sampler_2d(input: Tensor, grid: Tensor, interpolation_mode: _int, padding_mode: _int, align_corners: _bool) -> Tensor: ...\ndef grid_sampler_3d(input: Tensor, grid: Tensor, interpolation_mode: _int, padding_mode: _int, align_corners: _bool) -> Tensor: ...\ndef group_norm(input: Tensor, num_groups: _int, weight: Optional[Tensor]=None, bias: Optional[Tensor]=None, eps: _float=1e-05, cudnn_enabled: _bool=True) -> Tensor: ...\n@overload\ndef gru(input: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef gru(data: Tensor, batch_sizes: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[Tensor, Tensor]: ...\ndef gru_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Optional[Tensor]=None, b_hh: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef gt(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef gt(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef hamming_window(window_length: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef hamming_window(window_length: _int, periodic: _bool, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef hamming_window(window_length: _int, periodic: _bool, alpha: _float, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef hamming_window(window_length: _int, periodic: _bool, alpha: _float, beta: _float, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef hann_window(window_length: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef hann_window(window_length: _int, periodic: _bool, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef hardshrink(self: Tensor, lambd: Number=0.5) -> Tensor: ...\ndef histc(self: Tensor, bins: _int=100, min: Number=0, max: Number=0, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef hspmm(mat1: Tensor, mat2: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef ifft(self: Tensor, signal_ndim: _int, normalized: _bool=False) -> Tensor: ...\ndef imag(self: Tensor) -> Tensor: ...\n@overload\ndef index_add(self: Tensor, dim: _int, index: Tensor, source: Tensor) -> Tensor: ...\n@overload\ndef index_add(self: Tensor, dim: Union[str, None], index: Tensor, source: Tensor) -> Tensor: ...\n@overload\ndef index_copy(self: Tensor, dim: _int, index: Tensor, source: Tensor) -> Tensor: ...\n@overload\ndef index_copy(self: Tensor, dim: Union[str, None], index: Tensor, source: Tensor) -> Tensor: ...\n@overload\ndef index_fill(self: Tensor, dim: _int, index: Tensor, value: Number) -> Tensor: ...\n@overload\ndef index_fill(self: Tensor, dim: _int, index: Tensor, value: Tensor) -> Tensor: ...\n@overload\ndef index_fill(self: Tensor, dim: Union[str, None], index: Tensor, value: Number) -> Tensor: ...\n@overload\ndef index_fill(self: Tensor, dim: Union[str, None], index: Tensor, value: Tensor) -> Tensor: ...\ndef index_put(self: Tensor, indices: Optional[Union[Tuple[Tensor, ...], List[Tensor]]], values: Tensor, accumulate: _bool=False) -> Tensor: ...\ndef index_put_(self: Tensor, indices: Optional[Union[Tuple[Tensor, ...], List[Tensor]]], values: Tensor, accumulate: _bool=False) -> Tensor: ...\n@overload\ndef index_select(self: Tensor, dim: _int, index: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef index_select(self: Tensor, dim: Union[str, None], index: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef instance_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], use_input_stats: _bool, momentum: _float, eps: _float, cudnn_enabled: _bool) -> Tensor: ...\ndef int_repr(self: Tensor) -> Tensor: ...\ndef inverse(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef irfft(self: Tensor, signal_ndim: _int, normalized: _bool=False, onesided: _bool=True, signal_sizes: _size=()) -> Tensor: ...\ndef is_complex(self: Tensor) -> _bool: ...\ndef is_distributed(self: Tensor) -> _bool: ...\ndef is_floating_point(self: Tensor) -> _bool: ...\ndef is_grad_enabled() -> _bool: ...\ndef is_nonzero(self: Tensor) -> _bool: ...\ndef is_same_size(self: Tensor, other: Tensor) -> _bool: ...\ndef is_signed(self: Tensor) -> _bool: ...\ndef isclose(self: Tensor, other: Tensor, rtol: _float=1e-05, atol: _float=1e-08, equal_nan: _bool=False) -> Tensor: ...\ndef isfinite(self: Tensor) -> Tensor: ...\ndef isinf(self: Tensor) -> Tensor: ...\ndef isnan(self: Tensor) -> Tensor: ...\n@overload\ndef kthvalue(self: Tensor, k: _int, dim: _int=-1, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef kthvalue(self: Tensor, k: _int, dim: Union[str, None], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\ndef layer_norm(input: Tensor, normalized_shape: _size, weight: Optional[Tensor]=None, bias: Optional[Tensor]=None, eps: _float=1e-05, cudnn_enable: _bool=True) -> Tensor: ...\n@overload\ndef le(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef le(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef lerp(self: Tensor, end: Tensor, weight: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef lerp(self: Tensor, end: Tensor, weight: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef lgamma(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef linspace(start: Number, end: Number, steps: _int=100, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef log(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef log10(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef log10_(self: Tensor) -> Tensor: ...\ndef log1p(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef log1p_(self: Tensor) -> Tensor: ...\ndef log2(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef log2_(self: Tensor) -> Tensor: ...\ndef log_(self: Tensor) -> Tensor: ...\n@overload\ndef log_softmax(self: Tensor, dim: _int, dtype: Optional[_dtype]=None) -> Tensor: ...\n@overload\ndef log_softmax(self: Tensor, dim: Union[str, None], *, dtype: Optional[_dtype]=None) -> Tensor: ...\ndef logdet(self: Tensor) -> Tensor: ...\ndef logical_and(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef logical_not(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef logical_or(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef logical_xor(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef logspace(start: Number, end: Number, steps: _int=100, base: _float=10.0, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef logsumexp(self: Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef logsumexp(self: Tensor, dim: List[Union[str, None]], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef lstm(input: Tensor, hx: Union[Tuple[Tensor, ...], List[Tensor]], params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[Tensor, Tensor, Tensor]: ...\n@overload\ndef lstm(data: Tensor, batch_sizes: Tensor, hx: Union[Tuple[Tensor, ...], List[Tensor]], params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[Tensor, Tensor, Tensor]: ...\ndef lstm_cell(input: Tensor, hx: Union[Tuple[Tensor, ...], List[Tensor]], w_ih: Tensor, w_hh: Tensor, b_ih: Optional[Tensor]=None, b_hh: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\ndef lstsq(self: Tensor, A: Tensor, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef lt(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef lt(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef lu_solve(self: Tensor, LU_data: Tensor, LU_pivots: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef masked_fill(self: Tensor, mask: Tensor, value: Number) -> Tensor: ...\n@overload\ndef masked_fill(self: Tensor, mask: Tensor, value: Tensor) -> Tensor: ...\ndef masked_scatter(self: Tensor, mask: Tensor, source: Tensor) -> Tensor: ...\ndef masked_select(self: Tensor, mask: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef matmul(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef matrix_power(self: Tensor, n: _int) -> Tensor: ...\n@overload\ndef matrix_rank(self: Tensor, tol: _float, symmetric: _bool=False) -> Tensor: ...\n@overload\ndef matrix_rank(self: Tensor, symmetric: _bool=False) -> Tensor: ...\n@overload\ndef max(self: Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef max(self: Tensor, dim: Union[str, None], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef max(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef max(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef max_pool1d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tensor: ...\ndef max_pool1d_with_indices(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tuple[Tensor, Tensor]: ...\ndef max_pool2d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tensor: ...\ndef max_pool3d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tensor: ...\n@overload\ndef mean(self: Tensor, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef mean(self: Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef mean(self: Tensor, dim: List[Union[str, None]], keepdim: _bool=False, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef median(self: Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef median(self: Tensor, dim: Union[str, None], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef median(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef min(self: Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef min(self: Tensor, dim: Union[str, None], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef min(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef min(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef miopen_batch_norm(input: Tensor, weight: Tensor, bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: _bool, exponential_average_factor: _float, epsilon: _float) -> Tuple[Tensor, Tensor, Tensor]: ...\ndef miopen_convolution(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...\ndef miopen_convolution_transpose(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, output_padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...\ndef miopen_depthwise_convolution(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...\ndef miopen_rnn(input: Tensor, weight: Union[Tuple[Tensor, ...], List[Tensor]], weight_stride0: _int, hx: Tensor, cx: Optional[Tensor], mode: _int, hidden_size: _int, num_layers: _int, batch_first: _bool, dropout: _float, train: _bool, bidirectional: _bool, batch_sizes: _size, dropout_state: Optional[Tensor]) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]: ...\ndef mkldnn_adaptive_avg_pool2d(self: Tensor, output_size: Union[_int, _size]) -> Tensor: ...\ndef mkldnn_convolution(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, stride: _size, dilation: _size, groups: _int) -> Tensor: ...\ndef mkldnn_convolution_backward_weights(weight_size: _size, grad_output: Tensor, self: Tensor, padding: _size, stride: _size, dilation: _size, groups: _int, bias_defined: _bool) -> Tuple[Tensor, Tensor]: ...\ndef mkldnn_max_pool2d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tensor: ...\ndef mm(self: Tensor, mat2: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef mode(self: Tensor, dim: _int=-1, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef mode(self: Tensor, dim: Union[str, None], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\ndef mul(input: Union[Tensor, Number], other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\ndef multinomial(self: Tensor, num_samples: _int, replacement: _bool=False, *, generator: Generator=None, out: Optional[Tensor]=None) -> Tensor: ...\ndef mv(self: Tensor, vec: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef mvlgamma(self: Tensor, p: _int) -> Tensor: ...\n@overload\ndef narrow(self: Tensor, dim: _int, start: _int, length: _int) -> Tensor: ...\n@overload\ndef narrow(self: Tensor, dim: _int, start: Tensor, length: _int) -> Tensor: ...\ndef native_batch_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: _bool, momentum: _float, eps: _float, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor, Tensor]: ...\ndef native_layer_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], M: _int, N: _int, eps: _float) -> Tuple[Tensor, Tensor, Tensor]: ...\ndef native_norm(self: Tensor, p: Number=2) -> Tensor: ...\n@overload\ndef ne(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef ne(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef neg(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef neg_(self: Tensor) -> Tensor: ...\ndef norm_except_dim(v: Tensor, pow: _int=2, dim: _int=0) -> Tensor: ...\n@overload\ndef normal(mean: Tensor, std: _float=1, *, generator: Generator=None, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef normal(mean: _float, std: Tensor, *, generator: Generator=None, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef normal(mean: Tensor, std: Tensor, *, generator: Generator=None, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef normal(mean: _float, std: _float, size: _size, *, generator: Generator=None, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef nuclear_norm(self: Tensor, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef nuclear_norm(self: Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef numel(self: Tensor) -> _int: ...\n@overload\ndef ones(size: _size, *, names: Optional[List[Union[str, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef ones(*size: _int, names: Optional[List[Union[str, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef ones(size: _size, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef ones(*size: _int, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef ones_like(self: Tensor, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef orgqr(self: Tensor, input2: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef ormqr(self: Tensor, input2: Tensor, input3: Tensor, left: _bool=True, transpose: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef pairwise_distance(x1: Tensor, x2: Tensor, p: _float=2, eps: _float=1e-06, keepdim: _bool=False) -> Tensor: ...\ndef pdist(self: Tensor, p: _float=2) -> Tensor: ...\ndef pinverse(self: Tensor, rcond: _float=1e-15) -> Tensor: ...\ndef pixel_shuffle(self: Tensor, upscale_factor: _int) -> Tensor: ...\ndef poisson(self: Tensor, generator: Generator=None) -> Tensor: ...\ndef poisson_nll_loss(input: Tensor, target: Tensor, log_input: _bool, full: _bool, eps: _float, reduction: _int) -> Tensor: ...\ndef polygamma(n: _int, self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef pow(self: Tensor, exponent: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef pow(self: Tensor, exponent: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef pow(self: Number, exponent: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef prelu(self: Tensor, weight: Tensor) -> Tensor: ...\n@overload\ndef prod(self: Tensor, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef prod(self: Tensor, dim: _int, keepdim: _bool=False, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef prod(self: Tensor, dim: Union[str, None], keepdim: _bool=False, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...\ndef promote_types(type1: _dtype, type2: _dtype) -> _dtype: ...\ndef q_per_channel_axis(self: Tensor) -> _int: ...\ndef q_per_channel_scales(self: Tensor) -> Tensor: ...\ndef q_per_channel_zero_points(self: Tensor) -> Tensor: ...\ndef q_scale(self: Tensor) -> _float: ...\ndef q_zero_point(self: Tensor) -> _int: ...\ndef qr(self: Tensor, some: _bool=True, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\ndef quantize_per_channel(self: Tensor, scales: Tensor, zero_points: Tensor, axis: _int, dtype: _dtype) -> Tensor: ...\ndef quantize_per_tensor(self: Tensor, scale: _float, zero_point: _int, dtype: _dtype) -> Tensor: ...\ndef quantized_batch_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], mean: Tensor, var: Tensor, eps: _float, output_scale: _float, output_zero_point: _int) -> Tensor: ...\n@overload\ndef quantized_gru(input: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef quantized_gru(data: Tensor, batch_sizes: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[Tensor, Tensor]: ...\ndef quantized_gru_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tensor: ...\n@overload\ndef quantized_lstm(input: Tensor, hx: Union[Tuple[Tensor, ...], List[Tensor]], params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool, *, dtype: Optional[_dtype]=None, use_dynamic: _bool=False) -> Tuple[Tensor, Tensor, Tensor]: ...\n@overload\ndef quantized_lstm(data: Tensor, batch_sizes: Tensor, hx: Union[Tuple[Tensor, ...], List[Tensor]], params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, *, dtype: Optional[_dtype]=None, use_dynamic: _bool=False) -> Tuple[Tensor, Tensor, Tensor]: ...\ndef quantized_lstm_cell(input: Tensor, hx: Union[Tuple[Tensor, ...], List[Tensor]], w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tuple[Tensor, Tensor]: ...\ndef quantized_max_pool2d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tensor: ...\ndef quantized_rnn_relu_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tensor: ...\ndef quantized_rnn_tanh_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tensor: ...\n@overload\ndef rand(size: _size, *, names: Optional[List[Union[str, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef rand(*size: _int, names: Optional[List[Union[str, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef rand(size: _size, *, generator: Generator, names: Optional[List[Union[str, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef rand(*size: _int, generator: Generator, names: Optional[List[Union[str, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef rand(size: _size, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef rand(*size: _int, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef rand(size: _size, *, generator: Generator, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef rand(*size: _int, generator: Generator, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef rand_like(self: Tensor, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef randint(low: _int, high: _int, size: _size, *, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...\n@overload\ndef randint(high: _int, size: _size, *, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...\n@overload\ndef randint_like(self: Tensor, high: _int, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef randint_like(self: Tensor, low: _int, high: _int, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef randn(size: _size, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef randn(*size: _int, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef randn(size: _size, *, generator: Generator, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef randn(*size: _int, generator: Generator, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef randn(size: _size, *, names: Optional[List[Union[str, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef randn(*size: _int, names: Optional[List[Union[str, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef randn(size: _size, *, generator: Generator, names: Optional[List[Union[str, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef randn(*size: _int, generator: Generator, names: Optional[List[Union[str, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef randn_like(self: Tensor, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef randperm(n: _int, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef randperm(n: _int, *, generator: Generator, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef range(start: Number, end: Number, step: Number=1, *, out: Optional[Tensor]=None, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...\ndef real(self: Tensor) -> Tensor: ...\ndef reciprocal(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef reciprocal_(self: Tensor) -> Tensor: ...\ndef relu(self: Tensor) -> Tensor: ...\ndef relu_(self: Tensor) -> Tensor: ...\n@overload\ndef remainder(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef remainder(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef renorm(self: Tensor, p: Number, dim: _int, maxnorm: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef repeat_interleave(repeats: Tensor) -> Tensor: ...\n@overload\ndef repeat_interleave(self: Tensor, repeats: Tensor, dim: Optional[_int]=None) -> Tensor: ...\n@overload\ndef repeat_interleave(self: Tensor, repeats: _int, dim: Optional[_int]=None) -> Tensor: ...\ndef reshape(self: Tensor, shape: _size) -> Tensor: ...\ndef resize_as_(self: Tensor, the_template: Tensor, *, memory_format: Optional[memory_format]=None) -> Tensor: ...\n@overload\ndef result_type(tensor: Tensor, other: Tensor) -> _dtype: ...\n@overload\ndef result_type(tensor: Tensor, other: Number) -> _dtype: ...\n@overload\ndef result_type(scalar: Number, tensor: Tensor) -> _dtype: ...\n@overload\ndef result_type(scalar1: Number, scalar2: Number) -> _dtype: ...\ndef rfft(self: Tensor, signal_ndim: _int, normalized: _bool=False, onesided: _bool=True) -> Tensor: ...\n@overload\ndef rnn_relu(input: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef rnn_relu(data: Tensor, batch_sizes: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[Tensor, Tensor]: ...\ndef rnn_relu_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Optional[Tensor]=None, b_hh: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef rnn_tanh(input: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef rnn_tanh(data: Tensor, batch_sizes: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[Tensor, Tensor]: ...\ndef rnn_tanh_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Optional[Tensor]=None, b_hh: Optional[Tensor]=None) -> Tensor: ...\ndef roll(self: Tensor, shifts: Union[_int, _size], dims: Union[_int, _size]=()) -> Tensor: ...\ndef rot90(self: Tensor, k: _int=1, dims: _size=(0,1)) -> Tensor: ...\ndef round(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef round_(self: Tensor) -> Tensor: ...\ndef rrelu(self: Tensor, lower: Number=0.125, upper: Number=0.3333333333333333, training: _bool=False, generator: Generator=None) -> Tensor: ...\ndef rrelu_(self: Tensor, lower: Number=0.125, upper: Number=0.3333333333333333, training: _bool=False, generator: Generator=None) -> Tensor: ...\ndef rsqrt(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef rsqrt_(self: Tensor) -> Tensor: ...\n@overload\ndef rsub(self: Tensor, other: Tensor, *, alpha: Number=1) -> Tensor: ...\n@overload\ndef rsub(self: Tensor, other: Number, alpha: Number=1) -> Tensor: ...\ndef scalar_tensor(s: Number, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef scatter(self: Tensor, dim: _int, index: Tensor, src: Tensor) -> Tensor: ...\n@overload\ndef scatter(self: Tensor, dim: _int, index: Tensor, value: Number) -> Tensor: ...\n@overload\ndef scatter(self: Tensor, dim: Union[str, None], index: Tensor, src: Tensor) -> Tensor: ...\n@overload\ndef scatter(self: Tensor, dim: Union[str, None], index: Tensor, value: Number) -> Tensor: ...\n@overload\ndef scatter_add(self: Tensor, dim: _int, index: Tensor, src: Tensor) -> Tensor: ...\n@overload\ndef scatter_add(self: Tensor, dim: Union[str, None], index: Tensor, src: Tensor) -> Tensor: ...\n@overload\ndef select(self: Tensor, dim: Union[str, None], index: _int) -> Tensor: ...\n@overload\ndef select(self: Tensor, dim: _int, index: _int) -> Tensor: ...\ndef selu(self: Tensor) -> Tensor: ...\ndef selu_(self: Tensor) -> Tensor: ...\ndef set_flush_denormal(mode: _bool) -> _bool: ...\ndef set_num_interop_threads(num: _int) -> None: ...\ndef set_num_threads(num: _int) -> None: ...\ndef sigmoid(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef sigmoid_(self: Tensor) -> Tensor: ...\ndef sign(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef sin(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef sin_(self: Tensor) -> Tensor: ...\ndef sinh(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef sinh_(self: Tensor) -> Tensor: ...\ndef slogdet(self: Tensor) -> Tuple[Tensor, Tensor]: ...\ndef smm(self: Tensor, mat2: Tensor) -> Tensor: ...\n@overload\ndef softmax(self: Tensor, dim: _int, dtype: Optional[_dtype]=None) -> Tensor: ...\n@overload\ndef softmax(self: Tensor, dim: Union[str, None], *, dtype: Optional[_dtype]=None) -> Tensor: ...\ndef solve(self: Tensor, A: Tensor, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef sort(self: Tensor, dim: _int=-1, descending: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef sort(self: Tensor, dim: Union[str, None], descending: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\ndef sparse_coo_tensor(indices: Tensor, values: Union[Tensor,List], size: Optional[_size]=None, *, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef split_with_sizes(self: Tensor, split_sizes: _size, dim: _int=0) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...\ndef sqrt(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef sqrt_(self: Tensor) -> Tensor: ...\ndef square(self: Tensor) -> Tensor: ...\ndef square_(self: Tensor) -> Tensor: ...\n@overload\ndef squeeze(self: Tensor) -> Tensor: ...\n@overload\ndef squeeze(self: Tensor, dim: _int) -> Tensor: ...\n@overload\ndef squeeze(self: Tensor, dim: Union[str, None]) -> Tensor: ...\n@overload\ndef sspaddmm(self: Tensor, mat1: Tensor, mat2: Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef sspaddmm(beta: Number, self: Tensor, alpha: Number, mat1: Tensor, mat2: Tensor) -> Tensor: ...\n@overload\ndef sspaddmm(beta: Number, self: Tensor, mat1: Tensor, mat2: Tensor) -> Tensor: ...\ndef stack(tensors: Union[Tuple[Tensor, ...], List[Tensor]], dim: _int=0, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef std(self: Tensor, unbiased: _bool=True, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef std(self: Tensor, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef std(self: Tensor, dim: List[Union[str, None]], unbiased: _bool=True, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef std_mean(self: Tensor, unbiased: _bool=True) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef std_mean(self: Tensor, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef std_mean(self: Tensor, dim: List[Union[str, None]], unbiased: _bool=True, keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef sub(input: Union[Tensor, Number], other: Union[Tensor, Number], *, alpha: Optional[Number]=1, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef sub(self: Tensor, alpha: Number, other: Tensor) -> Tensor: ...\n@overload\ndef sub(self: Tensor, alpha: Number, other: Tensor, *, out: Tensor) -> Tensor: ...\n@overload\ndef sum(self: Tensor, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef sum(self: Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef sum(self: Tensor, dim: List[Union[str, None]], keepdim: _bool=False, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...\ndef svd(self: Tensor, some: _bool=True, compute_uv: _bool=True, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor, Tensor]: ...\ndef symeig(self: Tensor, eigenvectors: _bool=False, upper: _bool=True, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\ndef t(self: Tensor) -> Tensor: ...\ndef take(self: Tensor, index: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef tan(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef tan_(self: Tensor) -> Tensor: ...\ndef tanh(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef tanh_(self: Tensor) -> Tensor: ...\ndef tensor(data: Any, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...\ndef threshold(self: Tensor, threshold: Number, value: Number, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef threshold_(self: Tensor, threshold: Number, value: Number) -> Tensor: ...\ndef topk(self: Tensor, k: _int, dim: _int=-1, largest: _bool=True, sorted: _bool=True, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\ndef trace(self: Tensor) -> Tensor: ...\n@overload\ndef transpose(self: Tensor, dim0: _int, dim1: _int) -> Tensor: ...\n@overload\ndef transpose(self: Tensor, dim0: Union[str, None], dim1: Union[str, None]) -> Tensor: ...\n@overload\ndef trapz(y: Tensor, x: Tensor, *, dim: _int=-1) -> Tensor: ...\n@overload\ndef trapz(y: Tensor, *, dx: _float=1, dim: _int=-1) -> Tensor: ...\ndef triangular_solve(self: Tensor, A: Tensor, upper: _bool=True, transpose: _bool=False, unitriangular: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...\ndef tril(self: Tensor, diagonal: _int=0, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef tril_indices(row: _int, col: _int, offset: _int=0, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef triu(self: Tensor, diagonal: _int=0, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef triu_indices(row: _int, col: _int, offset: _int=0, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef true_divide(input: Union[Tensor, Number], other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\ndef trunc(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...\ndef trunc_(self: Tensor) -> Tensor: ...\n@overload\ndef unbind(self: Tensor, dim: _int=0) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...\n@overload\ndef unbind(self: Tensor, dim: Union[str, None]) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...\ndef unique_dim(self: Tensor, dim: _int, sorted: _bool=True, return_inverse: _bool=False, return_counts: _bool=False) -> Tuple[Tensor, Tensor, Tensor]: ...\ndef unsqueeze(self: Tensor, dim: _int) -> Tensor: ...\n@overload\ndef var(self: Tensor, unbiased: _bool=True, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef var(self: Tensor, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef var(self: Tensor, dim: List[Union[str, None]], unbiased: _bool=True, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...\n@overload\ndef var_mean(self: Tensor, unbiased: _bool=True) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef var_mean(self: Tensor, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef var_mean(self: Tensor, dim: List[Union[str, None]], unbiased: _bool=True, keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...\n@overload\ndef where(condition: Tensor, self: Tensor, other: Tensor) -> Tensor: ...\n@overload\ndef where(condition: Tensor) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...\ndef zero_(self: Tensor) -> Tensor: ...\n@overload\ndef zeros(size: _size, *, names: Optional[List[Union[str, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef zeros(*size: _int, names: Optional[List[Union[str, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef zeros(size: _size, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n@overload\ndef zeros(*size: _int, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\ndef zeros_like(self: Tensor, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...\n\nclass DoubleStorage(Storage): ...\nclass FloatStorage(Storage): ...\nclass LongStorage(Storage): ...\nclass IntStorage(Storage): ...\nclass ShortStorage(Storage): ...\nclass CharStorage(Storage): ...\nclass ByteStorage(Storage): ...\nclass BoolStorage(Storage): ...\nclass DoubleTensor(Tensor): ...\nclass FloatTensor(Tensor): ...\nclass LongTensor(Tensor): ...\nclass IntTensor(Tensor): ...\nclass ShortTensor(Tensor): ...\nclass CharTensor(Tensor): ...\nclass ByteTensor(Tensor): ...\nclass BoolTensor(Tensor): ...\n\nfloat32: dtype = ...\nfloat: dtype = ...\nfloat64: dtype = ...\ndouble: dtype = ...\nfloat16: dtype = ...\nbfloat16: dtype = ...\nhalf: dtype = ...\nuint8: dtype = ...\nint8: dtype = ...\nint16: dtype = ...\nshort: dtype = ...\nint32: dtype = ...\nint: dtype = ...\nint64: dtype = ...\nlong: dtype = ...\ncomplex32: dtype = ...\ncomplex64: dtype = ...\ncomplex128: dtype = ...\nquint8: dtype = ...\nqint8: dtype = ...\nqint32: dtype = ...\nbool: dtype = ...\n\n# Pure Python functions defined in torch/__init__.py\n\ndef typename(obj) -> str: ...\ndef is_tensor(obj) -> _bool: ...\ndef is_storage(obj) -> _bool: ...\ndef set_default_tensor_type(type) -> None: ...  # ick, what a bad legacy API\ndef set_default_dtype(d : _dtype) -> None: ...\ndef manager_path() -> str: ...\ndef compiled_with_cxx11_abi() -> _bool: ...\n\n# The return value of this function depends on the value of `as_tuple`,\n# (similar to `unique`, `lu`, etc.); as such, it is not\n# possible to type correctly\ndef nonzero(input: Tensor, *, out: Optional[Tensor]=None, as_tuple: Optional[_bool]=None): ...\n"], ["nn/__init__.pyi", "from .modules import *\nfrom .parameter import Parameter as Parameter\nfrom .parallel import DataParallel as DataParallel\nfrom . import init as init\nfrom . import utils as utils\nfrom . import functional as functional\nfrom . import parallel as parallel\n"], ["nn/modules/__init__.pyi", "from .module import Module as Module\nfrom .activation import CELU as CELU, ELU as ELU, GLU as GLU, GELU as GELU, Hardshrink as Hardshrink, \\\n    Hardtanh as Hardtanh, LeakyReLU as LeakyReLU, LogSigmoid as LogSigmoid, LogSoftmax as LogSoftmax, PReLU as PReLU, \\\n    RReLU as RReLU, ReLU as ReLU, ReLU6 as ReLU6, SELU as SELU, Sigmoid as Sigmoid, Softmax as Softmax, \\\n    Softmax2d as Softmax2d, Softmin as Softmin, Softplus as Softplus, Softshrink as Softshrink, Softsign as Softsign, \\\n    Tanh as Tanh, Tanhshrink as Tanhshrink, Threshold as Threshold, MultiheadAttention as MultiheadAttention\nfrom .adaptive import AdaptiveLogSoftmaxWithLoss as AdaptiveLogSoftmaxWithLoss\nfrom .batchnorm import BatchNorm1d as BatchNorm1d, BatchNorm2d as BatchNorm2d, BatchNorm3d as BatchNorm3d, \\\n    SyncBatchNorm as SyncBatchNorm\nfrom .container import Container as Container, ModuleDict as ModuleDict, ModuleList as ModuleList, \\\n    ParameterDict as ParameterDict, ParameterList as ParameterList, Sequential as Sequential\nfrom .conv import Conv1d as Conv1d, Conv2d as Conv2d, Conv3d as Conv3d, ConvTranspose1d as ConvTranspose1d, \\\n    ConvTranspose2d as ConvTranspose2d, ConvTranspose3d as ConvTranspose3d\nfrom .distance import CosineSimilarity as CosineSimilarity, PairwiseDistance as PairwiseDistance\nfrom .dropout import AlphaDropout as AlphaDropout, Dropout as Dropout, Dropout2d as Dropout2d, Dropout3d as Dropout3d, \\\n    FeatureAlphaDropout as FeatureAlphaDropout\nfrom .fold import Fold as Fold, Unfold as Unfold\nfrom .instancenorm import InstanceNorm1d as InstanceNorm1d, InstanceNorm2d as InstanceNorm2d, \\\n    InstanceNorm3d as InstanceNorm3d\nfrom .linear import Bilinear as Bilinear, Identity as Identity, Linear as Linear\nfrom .loss import BCELoss as BCELoss, BCEWithLogitsLoss as BCEWithLogitsLoss, CTCLoss as CTCLoss, \\\n    CosineEmbeddingLoss as CosineEmbeddingLoss, CrossEntropyLoss as CrossEntropyLoss, \\\n    HingeEmbeddingLoss as HingeEmbeddingLoss, KLDivLoss as KLDivLoss, L1Loss as L1Loss, MSELoss as MSELoss, \\\n    MarginRankingLoss as MarginRankingLoss, MultiLabelMarginLoss as MultiLabelMarginLoss, \\\n    MultiLabelSoftMarginLoss as MultiLabelSoftMarginLoss, MultiMarginLoss as MultiMarginLoss, NLLLoss as NLLLoss, \\\n    NLLLoss2d as NLLLoss2d, PoissonNLLLoss as PoissonNLLLoss, SmoothL1Loss as SmoothL1Loss, \\\n    SoftMarginLoss as SoftMarginLoss, TripletMarginLoss as TripletMarginLoss\nfrom .module import Module as Module\nfrom .normalization import CrossMapLRN2d as CrossMapLRN2d, GroupNorm as GroupNorm, LayerNorm as LayerNorm, \\\n    LocalResponseNorm as LocalResponseNorm\nfrom .padding import ConstantPad1d as ConstantPad1d, ConstantPad2d as ConstantPad2d, ConstantPad3d as ConstantPad3d, \\\n    ReflectionPad1d as ReflectionPad1d, ReflectionPad2d as ReflectionPad2d, ReplicationPad1d as ReplicationPad1d, \\\n    ReplicationPad2d as ReplicationPad2d, ReplicationPad3d as ReplicationPad3d, ZeroPad2d as ZeroPad2d\nfrom .pixelshuffle import PixelShuffle as PixelShuffle\nfrom .pooling import AdaptiveAvgPool1d as AdaptiveAvgPool1d, AdaptiveAvgPool2d as AdaptiveAvgPool2d, \\\n    AdaptiveAvgPool3d as AdaptiveAvgPool3d, AdaptiveMaxPool1d as AdaptiveMaxPool1d, \\\n    AdaptiveMaxPool2d as AdaptiveMaxPool2d, AdaptiveMaxPool3d as AdaptiveMaxPool3d, AvgPool1d as AvgPool1d, \\\n    AvgPool2d as AvgPool2d, AvgPool3d as AvgPool3d, FractionalMaxPool2d as FractionalMaxPool2d, \\\n    FractionalMaxPool3d as FractionalMaxPool3d, LPPool1d as LPPool1d, LPPool2d as LPPool2d, MaxPool1d as MaxPool1d, \\\n    MaxPool2d as MaxPool2d, MaxPool3d as MaxPool3d, MaxUnpool1d as MaxUnpool1d, MaxUnpool2d as MaxUnpool2d, \\\n    MaxUnpool3d as MaxUnpool3d\nfrom .rnn import GRU as GRU, GRUCell as GRUCell, LSTM as LSTM, LSTMCell as LSTMCell, RNN as RNN, RNNBase as RNNBase, \\\n    RNNCell as RNNCell, RNNCellBase as RNNCellBase\nfrom .sparse import Embedding as Embedding, EmbeddingBag as EmbeddingBag\nfrom .upsampling import Upsample as Upsample, UpsamplingBilinear2d as UpsamplingBilinear2d, \\\n    UpsamplingNearest2d as UpsamplingNearest2d\nfrom .transformer import Transformer as Transformer, TransformerEncoder as TransformerEncoder, \\\n    TransformerDecoder as TransformerDecoder, TransformerEncoderLayer as TransformerEncoderLayer, \\\n    TransformerDecoderLayer as TransformerDecoderLayer\n"], ["nn/parallel/__init__.pyi", "from .data_parallel import DataParallel as DataParallel, data_parallel as data_parallel\nfrom .distributed import DistributedDataParallel as DistributedDataParallel\nfrom .parallel_apply import parallel_apply as parallel_apply\nfrom .replicate import replicate as replicate\nfrom .scatter_gather import gather as gather, scatter as scatter\n"], ["autograd/__init__.pyi", "from typing import Any, Callable, Union, Tuple, Sequence, Optional\nfrom .. import Tensor\nfrom .grad_mode import no_grad as no_grad, enable_grad as enable_grad, \\\n    set_grad_enabled as set_grad_enabled\nfrom . import profiler\n\n# The Variable API has been deprecated.\n# Variable(tensor) and Variable(tensor, requires_grad) still work, but they return Tensors instead of Variables.\ndef Variable(tensor: Tensor, requires_grad: bool=...) -> Tensor: ...\n\nclass Function:\n    @staticmethod\n    def forward(ctx: Any, *args: Any, **kwargs: Any) -> Any: ...\n    @staticmethod\n    def backward(ctx: Any, *grad_outputs: Any) -> Any: ...\n\nclass NestedIOFunction(Function):\n    # The 'type: ignore' statements are needed here because these functions are declared as '@staticmethod' in the\n    # superclass (Function) but are instance methods here, which mypy reports as incomptabile.\n    def backward(self, *gradients: Any) -> Any: ...  # type: ignore\n    def forward(self, *args: Any) -> tuple: ...  # type: ignore\n    def save_for_backward(self, *args: Any) -> None:...\n    def mark_dirty(self, *args: Any, **kwargs: Any) -> None:...\n    def mark_non_differentiable(self, *args: Any, **kwargs: Any) -> None: ...\n    def forward_extended(self, *input: Any) -> None:...\n    def backward_extended(self, *grad_output: Any) -> None: ...\n\n# 'func' accepts a vararg of tensors, which isn't expressable in the type system at the moment.\n# If https://mypy.readthedocs.io/en/latest/additional_features.html?highlight=callable#extended-callable-types is accepted,\n# the '...' first argument of Callable can be replaced with VarArg(Tensor).\n# For now, we permit any input.\ndef gradcheck(func: Callable[..., Union[Tensor, Tuple[Tensor, ...]]], inputs: Union[Tensor, Tuple[Tensor, ...]], eps: float=..., atol: float=..., rtol: float=..., raise_exception: bool=..., check_sparse_nnz: bool=...) -> bool: ...\ndef gradgradcheck(func: Callable[..., Union[Tensor, Tuple[Tensor, ...]]], inputs: Union[Tensor, Tuple[Tensor, ...]], eps: float=..., atol: float=..., rtol: float=..., gen_non_contig_grad_outputs: bool=..., raise_exception: bool=...) -> bool: ...\n\nclass detect_anomaly:\n    def __enter__(self) -> None: ...\n    def __exit__(self, *args: Any) -> bool: ...\n\nclass set_detect_anomaly:\n    def __init__(self, mode: bool) -> None: ...\n    def __enter__(self) -> None:...\n    def __exit__(self, *args: Any) -> bool: ...\n\n_TensorOrTensors = Union[Tensor, Sequence[Tensor]]\ndef backward(tensors: _TensorOrTensors, grad_tensors: Optional[_TensorOrTensors]=..., retain_graph: Optional[bool]=..., create_graph: bool=...) -> None: ...\ndef grad(outputs: _TensorOrTensors, inputs: _TensorOrTensors, grad_outputs: Optional[_TensorOrTensors]=..., retain_graph: Optional[bool]=..., create_graph: bool=..., only_inputs: bool=..., allow_unused: bool=...) -> Tuple[Tensor, ...]: ...\n"], ["optim/__init__.pyi", "from . import lr_scheduler as lr_scheduler\nfrom .adadelta import Adadelta\nfrom .adagrad import Adagrad\nfrom .adam import Adam as Adam\nfrom .adamax import Adamax\nfrom .adamw import AdamW as AdamW\nfrom .asgd import ASGD\nfrom .lbfgs import LBFGS\nfrom .optimizer import Optimizer\nfrom .rmsprop import RMSprop\nfrom .rprop import Rprop\nfrom .sgd import SGD as SGD\nfrom .sparse_adam import SparseAdam\n"], ["cuda/__init__.pyi", "from typing import Optional, Tuple, Union\nfrom .. import device as _device\n\ndef is_available() -> bool: ...\ndef init() -> None: ...\n\nclass cudaStatus:\n    SUCCESS: int\n    ERROR_NOT_READY: int\n\nclass CudaError:\n    def __init__(self, code: int) -> None: ...\n\nclass _CudaDeviceProperties:\n    name: str\n    major: int\n    minor: int\n    multi_processor_count: int\n    total_memory: int\n    is_integrated: int\n    is_multi_gpu_board: int\n\n_device_t = Union[_device, int]\n\ndef check_error(res: int) -> None: ...\ndef device_count() -> int: ...\ndef empty_cache() -> None: ...\ndef synchronize(device: _device_t) -> None: ...\ndef set_device(device: _device_t) -> None: ...\ndef get_device_capability(device: Optional[_device_t]=...) -> Tuple[int, int]: ...\ndef get_device_name(device: Optional[_device_t]=...) -> str: ...\ndef get_device_properties(device: _device_t) -> _CudaDeviceProperties: ...\ndef current_device() -> int: ...\ndef memory_allocated(device: Optional[_device_t]=...) -> int: ...\ndef max_memory_allocated(device: Optional[_device_t]=...) -> int: ...\ndef reset_max_memory_allocated(device: Optional[_device_t]=...) -> None: ...\ndef memory_cached(device: Optional[_device_t]=...) -> int: ...\ndef max_memory_cached(device: Optional[_device_t]=...) -> int: ...\ndef reset_max_memory_cached(device: Optional[_device_t]=...) -> None: ...\ndef set_rng_state(new_state): ...\ndef get_rng_state(): ...\n"], ["utils/data/__init__.pyi", "from .sampler import Sampler as Sampler, SequentialSampler as SequentialSampler, RandomSampler as RandomSampler, \\\n    SubsetRandomSampler as SubsetRandomSampler, WeightedRandomSampler as WeightedRandomSampler, BatchSampler as BatchSampler\nfrom .distributed import DistributedSampler as DistributedSampler\nfrom .dataset import Dataset as Dataset, TensorDataset as TensorDataset, ConcatDataset as ConcatDataset, \\\n    Subset as Subset, random_split as random_split, IterableDataset as IterableDataset, \\\n    ChainDataset as ChainDataset\nfrom .dataloader import DataLoader as DataLoader, get_worker_info as get_worker_info\n"]]